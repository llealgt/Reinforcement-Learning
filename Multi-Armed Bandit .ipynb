{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: multi-armed bandit with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandits = [0.2,0,-0.2,-5]\n",
    "num_bandits = len(bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pullBandit(banditNumber):\n",
    "    bandit = bandits[banditNumber]\n",
    "    result = np.random.randn(1)\n",
    "    \n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "best_action = tf.argmax(weights,0)\n",
    "\n",
    "selected_action = tf.placeholder(shape = [1], dtype=tf.int32)\n",
    "selected_action_reward = tf.placeholder(shape = [1],dtype=tf.float32)\n",
    "selected_action_weight = tf.slice(weights,selected_action,[1])\n",
    "advantage = selected_action_reward\n",
    "loss = -(tf.log(selected_action_weight)*advantage)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "explore_probability = 0.1\n",
    "stats_print_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 ,bandits rewards:[ 1.  0.  0.  0.]\n",
      "Step 50 ,bandits rewards:[-1.  8.  0.  0.]\n",
      "Step 100 ,bandits rewards:[ -1.   2.   0.  26.]\n",
      "Step 150 ,bandits rewards:[ -1.   1.   1.  74.]\n",
      "Step 200 ,bandits rewards:[  -1.    1.    1.  122.]\n",
      "Step 250 ,bandits rewards:[  -1.    4.    1.  169.]\n",
      "Step 300 ,bandits rewards:[  -1.    3.    1.  216.]\n",
      "Step 350 ,bandits rewards:[  -3.    1.    0.  259.]\n",
      "Step 400 ,bandits rewards:[  -3.    0.   -1.  305.]\n",
      "Step 450 ,bandits rewards:[  -5.    1.    1.  350.]\n",
      "Step 500 ,bandits rewards:[  -5.    3.    2.  395.]\n",
      "Step 550 ,bandits rewards:[  -5.    4.    1.  441.]\n",
      "Step 600 ,bandits rewards:[  -5.    4.    2.  488.]\n",
      "Step 650 ,bandits rewards:[  -6.    6.    3.  532.]\n",
      "Step 700 ,bandits rewards:[  -5.    6.    3.  581.]\n",
      "Step 750 ,bandits rewards:[  -4.    6.    3.  628.]\n",
      "Step 800 ,bandits rewards:[  -2.    8.    4.  673.]\n",
      "Step 850 ,bandits rewards:[  -1.   11.    5.  718.]\n",
      "Step 900 ,bandits rewards:[  -2.   12.    5.  764.]\n",
      "Step 950 ,bandits rewards:[  -3.   12.    6.  812.]\n",
      "Final rewards:[  -2.   10.    6.  856.]\n",
      "The agent thinks bandit 4 is the best \n",
      "...The agent got the right answer\n"
     ]
    }
   ],
   "source": [
    "initialize = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(initialize)\n",
    "    \n",
    "    for step in range(total_episodes):\n",
    "        if np.random.rand(1) < explore_probability:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = session.run(best_action)\n",
    "            \n",
    "        reward = pullBandit(action)\n",
    "        \n",
    "        feed_dict = {selected_action:[action],selected_action_reward:[reward]}\n",
    "        _,selected_weight,all_weights = session.run([update,selected_action_weight,weights],\n",
    "                                                   feed_dict=feed_dict)\n",
    "        \n",
    "        total_reward[action]+=reward \n",
    "        \n",
    "        if step% stats_print_steps == 0:\n",
    "            print(\"Step \"+str(step)+\" ,bandits rewards:\"+str(total_reward))\n",
    "    print(\"Final rewards:\"+str(total_reward))\n",
    "        \n",
    "    print(\"The agent thinks bandit \"+str(np.argmax(all_weights)+1)+ \" is the best \")\n",
    "    if np.argmax(all_weights) == np.argmax(-np.array(bandits)):\n",
    "        print(\"...The agent got the right answer\")\n",
    "    else:\n",
    "        print(\"...The agent got the wrong answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
