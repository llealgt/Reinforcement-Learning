{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: multi-armed bandit with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For this simple example, 4 bandits will be used, the value of every bandit its used to calculate its reward or punishment\n",
    "# In a different application or business case, the bandits list could be used just as a reference of avaliable resources or items\n",
    "bandits = [0.2,0,-0.2,-5]\n",
    "num_bandits = len(bandits) #4 bandits in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pullBandit(banditNumber) function simulates an action on the given bandit(banditNumber) and returns a reward(or punishment) for this action. \n",
    "In this example, the reward or punishment is calculated by a simple random number that depends on the bandit value, the more negative the bandit value the more probability of getting reward, the more positive the bandit value the more probability of getting a punishment so for this 4 bandits example, the last bandit its the best one and the first bandit its the worst as seen in bandits list :\n",
    "\n",
    "bandits = [0.2,0,-0.2,-5]\n",
    "\n",
    "### For a different application or different business case: \n",
    "The problem modeling changes to 1 bandit per resource(or item), and every resource(or item) provides a reward or punishment if its used(or activated) , this means the pullBandit function logic needs to change in order to provide a way to get  the reward( or punishment) associated with the given bandit(resource or item) ,for example a monetary performance indicator(money earns and losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the reward or punishment of using the given bandit\n",
    "def pullBandit(banditNumber):\n",
    "    bandit = bandits[banditNumber]\n",
    "    result = np.random.randn(1)\n",
    "    \n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "The model consists on a single output layer neural network , with 1 neuron(and its weight) per bandit ,the untuition behind it its that for every bandit ,the neurons weight means how good the agent has learned the bandit is, so the biggest weight corresponds to which bandit the agent learned as being the one that produces best results,this means the best action to take(bandit to pull) at every step its the one corresponding to the biggest weight.\n",
    "\n",
    "For every step(or episode)   the agent performs an exploration/exploration tradeoff ,this means that with a given probability \"e\" it will explore how a random bandit(action) performs to adquire new knowledge, and with a probability of 1-e , it will exploit the bandit that it has learned to be the one with the best rewards so the selected action depends on the agent being exploring or exploiting . \n",
    "\n",
    "The neurons weights are trained using gradient descent with a loss function of:\n",
    "\n",
    "__Loss = -log(Policy)*Advantage__\n",
    "\n",
    "Where:\n",
    "\n",
    "* Policy :is the weight of the selected action(pulled bandit) at every training step.\n",
    "* Advantage: its a measurment of how good the selected action was compared to some baseline, in this example to keep it simple, the advantage its the same as the reward of pulling the selected bandith(executing the selected action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#one weight per bandit , the biggest weight corresponds to the bandit with better results.\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "best_action = tf.argmax(weights,0)\n",
    "\n",
    "#At every episode the agent action(or bandit pulled) will be selected depending on its decision  \n",
    "#to exploit the bandit it has learned to be best action(with 1-e probability) or to \n",
    "#explore other bandits to learn and adquire new knowledge(with e probability)\n",
    "selected_action = tf.placeholder(shape = [1], dtype=tf.int32) #the selected action\n",
    "selected_action_reward = tf.placeholder(shape = [1],dtype=tf.float32) #the reward of executing the selected action\n",
    "selected_action_weight = tf.slice(weights,selected_action,[1]) #the score the agent gives to the selected action\n",
    "\n",
    "advantage = selected_action_reward #the advantage measure to calculate the loss(to compare the selected action to a baseline)\n",
    "loss = -(tf.log(selected_action_weight)*advantage) #the loss function to minimize \n",
    "\n",
    "# minimize loss function with gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_episodes = 1100 #number of steps\n",
    "total_reward = np.zeros(num_bandits) #total reward of every bandith\n",
    "explore_probability = 0.1 #probability to explore and get new knowledge\n",
    "stats_print_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 ,bandits rewards:[ 1.  0.  0.  0.]\n",
      "Step 50 ,bandits rewards:[ -1.  -1.   0.  45.]\n",
      "Step 100 ,bandits rewards:[ -2.  -3.  -1.  91.]\n",
      "Step 150 ,bandits rewards:[  -2.   -5.   -1.  139.]\n",
      "Step 200 ,bandits rewards:[  -2.   -5.   -1.  189.]\n",
      "Step 250 ,bandits rewards:[  -3.   -5.   -2.  233.]\n",
      "Step 300 ,bandits rewards:[  -2.   -5.   -1.  279.]\n",
      "Step 350 ,bandits rewards:[  -2.   -2.   -1.  322.]\n",
      "Step 400 ,bandits rewards:[  -1.   -2.   -1.  367.]\n",
      "Step 450 ,bandits rewards:[   0.   -2.   -3.  414.]\n",
      "Step 500 ,bandits rewards:[   0.   -2.   -1.  460.]\n",
      "Step 550 ,bandits rewards:[   0.   -4.    0.  505.]\n",
      "Step 600 ,bandits rewards:[   0.   -5.    0.  554.]\n",
      "Step 650 ,bandits rewards:[   0.   -5.    1.  603.]\n",
      "Step 700 ,bandits rewards:[   0.   -5.    2.  650.]\n",
      "Step 750 ,bandits rewards:[  -2.   -6.    2.  697.]\n",
      "Step 800 ,bandits rewards:[  -3.   -8.    5.  741.]\n",
      "Step 850 ,bandits rewards:[  -4.  -10.    4.  787.]\n",
      "Step 900 ,bandits rewards:[  -5.   -9.    4.  835.]\n",
      "Step 950 ,bandits rewards:[  -7.   -8.    5.  881.]\n",
      "Step 1000 ,bandits rewards:[  -9.   -9.    6.  927.]\n",
      "Step 1050 ,bandits rewards:[  -8.   -8.    9.  972.]\n",
      "Final rewards:[   -7.    -9.    10.  1016.]\n",
      "The agent thinks bandit 4 is the best \n",
      "...The agent got the right answer\n"
     ]
    }
   ],
   "source": [
    "initialize = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(initialize)\n",
    "    \n",
    "    for step in range(total_episodes):\n",
    "        # define if explore or exploit\n",
    "        if np.random.rand(1) < explore_probability:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = session.run(best_action)\n",
    "            \n",
    "        #get the reward of the given action\n",
    "        reward = pullBandit(action)\n",
    "        \n",
    "        #perform a gradient descent run  to update weights by minimizing loss for the selected action and its rewad\n",
    "        feed_dict = {selected_action:[action],selected_action_reward:[reward]}\n",
    "        _,selected_weight,all_weights = session.run([update,selected_action_weight,weights],\n",
    "                                                   feed_dict=feed_dict)\n",
    "        #increase the total reward of the selected action\n",
    "        total_reward[action]+=reward \n",
    "        \n",
    "        if step% stats_print_steps == 0:\n",
    "            print(\"Step \"+str(step)+\" ,bandits rewards:\"+str(total_reward))\n",
    "    print(\"Final rewards:\"+str(total_reward))\n",
    "        \n",
    "    print(\"The agent thinks bandit \"+str(np.argmax(all_weights)+1)+ \" is the best \")\n",
    "    if np.argmax(all_weights) == np.argmax(-np.array(bandits)):\n",
    "        print(\"...The agent got the right answer\")\n",
    "    else:\n",
    "        print(\"...The agent got the wrong answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
