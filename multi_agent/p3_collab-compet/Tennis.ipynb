{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple,deque\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.09000000171363354\n",
      "Score (max over agents) from episode 5: 0.09000000171363354\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations        # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_BUFFER_SIZE = int(1e6)\n",
    "VISUALIZE_EVERY = 50 #how many episodes to train before watching unity env simulation\n",
    "PRINT_EVERY = 10 #how many steps to train before printing training info\n",
    "DESIRED_EPISODES_AVERAGE = 100\n",
    "DESIRED_AVERAGE_SCORE = 0.5\n",
    "TRAIN_EVERY = 5\n",
    "BATCH_SIZE = 512\n",
    "UPDATE_TARGET_NETWORK_EVERY = 20\n",
    "TAU = 1e-3 \n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples for experience replay\"\"\"\n",
    "    \n",
    "    def __init__(self,action_size,buffer_size):\n",
    "        \"\"\"Initialize a ReplayBuffer objectParams\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\",field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        \"Add a new experience tuple to replay buffer\"\n",
    "        new_experience = self.experience(state,action,reward,next_state,done)\n",
    "        self.memory.append(new_experience)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        \"Get a sample of the buffer of size=batch size\"\n",
    "        experience_batch = random.sample(self.memory,k=batch_size)\n",
    "        \n",
    "        states =  torch.from_numpy(np.vstack([e.state  for e in experience_batch if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experience_batch if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experience_batch if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experience_batch if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experience_batch if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"\n",
    "        Orstein-Ulenbeck process\n",
    "        used to add noise to actor selections for adding exploratoion\n",
    "    \"\"\"\n",
    "    def __init__(self,size,mu=0.0,theta=0.15,sigma=0.2):\n",
    "        self.size = size\n",
    "        self.mu = mu*np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    \"\"\"Used to set the initial weights of hidden layers\"\"\"\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one actor will be shared by the 2 players, trained by self-play\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        super(Actor,self).__init__()\n",
    "        self.action_size = action_size\n",
    "        #self.bn=   torch.nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = torch.nn.Linear(state_size,400)\n",
    "        self.fc2 = torch.nn.Linear(400,300)\n",
    "        self.fc_output = torch.nn.Linear(300,action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x = self.bn(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        y = F.tanh(self.fc_output(x))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc_output.weight.data.uniform_(-0.0001, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one critic will be shared by the 2 players, trianed by self-play\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        super(Critic,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_size+ action_size,400) #use actions of both agent\n",
    "        self.fc2 = torch.nn.Linear(400,300)\n",
    "        self.fc_output = torch.nn.Linear(300,1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self,x,actions):\n",
    "        x = torch.cat((x,actions),dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        y = self.fc_output(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc_output.weight.data.uniform_(-3e-4, 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size,actor_local,replay_buffer,device):\n",
    "        self.action_size  = action_size\n",
    "        self.actor_local = actor_local\n",
    "        self.device = device\n",
    "        \n",
    "        self.replay_buffer = replay_buffer\n",
    "        \n",
    "        self.noise = OUNoise(size=action_size)\n",
    "        \n",
    "    def act(self,state):\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).detach().to(self.device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "            \n",
    "        self.actor_local.train()\n",
    "        noise = self.noise.sample()\n",
    "        action = np.clip(action + noise ,-1,1)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network(local_network,target_network,tau):\n",
    "    for target_param,local_param in zip(target_network.parameters(),local_network.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0 - tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(action_size,REPLAY_BUFFER_SIZE)\n",
    "\n",
    "actor_local = Actor(state_size,action_size).to(device)\n",
    "critic_local = Critic(state_size,action_size).to(device)\n",
    "\n",
    "actor_target = Actor(state_size,action_size).to(device)\n",
    "critic_target = Critic(state_size,action_size).to(device)\n",
    "\n",
    "update_target_network(actor_local,actor_target,1.0)\n",
    "update_target_network(critic_local,critic_target,1.0)\n",
    "\n",
    "critic_optimizer = optim.Adam(critic_local.parameters(),lr=1e-4 )\n",
    "actor_optimizer = optim.Adam(actor_local.parameters(),lr=1e-3)\n",
    "\n",
    "agent1 = Agent(state_size,action_size,actor_local,replay_buffer,device)\n",
    "agent2 = Agent(state_size,action_size,actor_local,replay_buffer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size=32,step_num=0,print_every = 100,critic_losses=[],actor_losses = []):\n",
    "    (states, actions, rewards, next_states, dones) = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    #actor_target.eval()\n",
    "    #critic_target.eval()\n",
    "    \n",
    "    next_target_actions = actor_target(next_states)\n",
    "    with torch.no_grad():\n",
    "        #next_target_actions = actor_target(next_states)\n",
    "        target_q_values = critic_target(next_states,next_target_actions)\n",
    "    target_return = rewards + (0.99*target_q_values*(1-dones))\n",
    "    \n",
    "    #train critic\n",
    "    #critic_local.train()\n",
    "    \n",
    "    current_critic_estimate = critic_local(states,actions)\n",
    "    critic_mse_loss = F.mse_loss(current_critic_estimate,target_return.detach())\n",
    "    \n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_mse_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(critic_local.parameters(),1)\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    critic_losses.append(critic_mse_loss)\n",
    "    \n",
    "    #train actor\n",
    "    local_actions = actor_local(states)\n",
    "    actor_loss = -critic_local(states,local_actions).mean()\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    actor_losses.append(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:10 steps:16 episode_reward:0.0 last n mean rew.0.010000000149011612,individual episode rew.[-0.01  0.  ]\n",
      "Episode:20 steps:14 episode_reward:0.0 last n mean rew.0.005000000074505806,individual episode rew.[ 0.   -0.01]\n",
      "Episode:30 steps:14 episode_reward:0.0 last n mean rew.0.0063333334401249886,individual episode rew.[-0.01  0.  ]\n",
      "Episode:40 steps:15 episode_reward:0.0 last n mean rew.0.004750000080093741,individual episode rew.[-0.01  0.  ]\n",
      "Episode:50 steps:15 episode_reward:0.0 last n mean rew.0.009600000157952308,individual episode rew.[ 0.   -0.01]\n",
      "Episode:60 steps:14 episode_reward:0.0 last n mean rew.0.0125000002173086,individual episode rew.[-0.01  0.  ]\n",
      "Episode:70 steps:15 episode_reward:0.0 last n mean rew.0.010714285900550229,individual episode rew.[-0.01  0.  ]\n",
      "Episode:80 steps:14 episode_reward:0.0 last n mean rew.0.01050000018440187,individual episode rew.[-0.01  0.  ]\n",
      "Episode:90 steps:14 episode_reward:0.0 last n mean rew.0.011333333535326852,individual episode rew.[-0.01  0.  ]\n",
      "Episode:100 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.012900000233203172,individual episode rew.[0.   0.09]\n",
      "Episode:110 steps:15 episode_reward:0.0 last n mean rew.0.012800000235438347,individual episode rew.[ 0.   -0.01]\n",
      "Episode:120 steps:15 episode_reward:0.0 last n mean rew.0.013800000250339508,individual episode rew.[ 0.   -0.01]\n",
      "Episode:130 steps:30 episode_reward:0.10000000149011612 last n mean rew.0.01790000030770898,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:140 steps:14 episode_reward:0.0 last n mean rew.0.0199000003375113,individual episode rew.[-0.01  0.  ]\n",
      "Episode:150 steps:14 episode_reward:0.0 last n mean rew.0.01790000030770898,individual episode rew.[ 0.   -0.01]\n",
      "Episode:160 steps:14 episode_reward:0.0 last n mean rew.0.01810000030323863,individual episode rew.[-0.01  0.  ]\n",
      "Episode:170 steps:14 episode_reward:0.0 last n mean rew.0.01910000031813979,individual episode rew.[-0.01  0.  ]\n",
      "Episode:180 steps:15 episode_reward:0.0 last n mean rew.0.018200000301003456,individual episode rew.[ 0.   -0.01]\n",
      "Episode:190 steps:15 episode_reward:0.0 last n mean rew.0.017400000281631946,individual episode rew.[ 0.   -0.01]\n",
      "Episode:200 steps:14 episode_reward:0.0 last n mean rew.0.0157000002451241,individual episode rew.[ 0.   -0.01]\n",
      "Episode:210 steps:15 episode_reward:0.0 last n mean rew.0.015800000242888926,individual episode rew.[-0.01  0.  ]\n",
      "Episode:220 steps:14 episode_reward:0.0 last n mean rew.0.014800000227987766,individual episode rew.[-0.01  0.  ]\n",
      "Episode:230 steps:15 episode_reward:0.0 last n mean rew.0.012800000198185444,individual episode rew.[ 0.   -0.01]\n",
      "Episode:240 steps:14 episode_reward:0.0 last n mean rew.0.012700000200420619,individual episode rew.[ 0.   -0.01]\n",
      "Episode:250 steps:13 episode_reward:0.0 last n mean rew.0.012800000198185444,individual episode rew.[ 0.   -0.01]\n",
      "Episode:260 steps:14 episode_reward:0.0 last n mean rew.0.012800000198185444,individual episode rew.[-0.01  0.  ]\n",
      "Episode:270 steps:14 episode_reward:0.0 last n mean rew.0.012800000198185444,individual episode rew.[-0.01  0.  ]\n",
      "Episode:280 steps:14 episode_reward:0.0 last n mean rew.0.013800000213086605,individual episode rew.[-0.01  0.  ]\n",
      "Episode:290 steps:14 episode_reward:0.0 last n mean rew.0.014700000230222941,individual episode rew.[-0.01  0.  ]\n",
      "Episode:300 steps:14 episode_reward:0.0 last n mean rew.0.014700000230222941,individual episode rew.[-0.01  0.  ]\n",
      "Episode:310 steps:14 episode_reward:0.0 last n mean rew.0.01370000021532178,individual episode rew.[-0.01  0.  ]\n",
      "Episode:320 steps:14 episode_reward:0.0 last n mean rew.0.014600000232458115,individual episode rew.[ 0.   -0.01]\n",
      "Episode:330 steps:14 episode_reward:0.0 last n mean rew.0.012600000202655792,individual episode rew.[-0.01  0.  ]\n",
      "Episode:340 steps:14 episode_reward:0.0 last n mean rew.0.011700000185519456,individual episode rew.[-0.01  0.  ]\n",
      "Episode:350 steps:14 episode_reward:0.0 last n mean rew.0.012600000202655792,individual episode rew.[ 0.   -0.01]\n",
      "Episode:360 steps:13 episode_reward:0.0 last n mean rew.0.009700000155717134,individual episode rew.[ 0.   -0.01]\n",
      "Episode:370 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.012500000204890966,individual episode rew.[0.   0.09]\n",
      "Episode:380 steps:14 episode_reward:0.0 last n mean rew.0.017400000281631946,individual episode rew.[ 0.   -0.01]\n",
      "Episode:390 steps:14 episode_reward:0.0 last n mean rew.0.016400000266730785,individual episode rew.[ 0.   -0.01]\n",
      "Episode:400 steps:14 episode_reward:0.0 last n mean rew.0.019400000311434268,individual episode rew.[-0.01  0.  ]\n",
      "Episode:410 steps:32 episode_reward:0.10000000149011612 last n mean rew.0.024200000390410422,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:420 steps:19 episode_reward:0.09000000357627869 last n mean rew.0.027100000455975534,individual episode rew.[0.09 0.  ]\n",
      "Episode:430 steps:15 episode_reward:0.0 last n mean rew.0.029100000485777856,individual episode rew.[ 0.   -0.01]\n",
      "Episode:440 steps:31 episode_reward:0.10000000149011612 last n mean rew.0.029100000485777856,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:450 steps:15 episode_reward:0.0 last n mean rew.0.029200000483542682,individual episode rew.[ 0.   -0.01]\n",
      "Episode:460 steps:31 episode_reward:0.10000000149011612 last n mean rew.0.0331000005453825,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:470 steps:13 episode_reward:0.0 last n mean rew.0.03330000054091215,individual episode rew.[ 0.   -0.01]\n",
      "Episode:480 steps:14 episode_reward:0.0 last n mean rew.0.029300000481307507,individual episode rew.[ 0.   -0.01]\n",
      "Episode:490 steps:15 episode_reward:0.0 last n mean rew.0.028400000464171172,individual episode rew.[ 0.   -0.01]\n",
      "Episode:500 steps:14 episode_reward:0.0 last n mean rew.0.027300000451505185,individual episode rew.[-0.01  0.  ]\n",
      "Episode:510 steps:30 episode_reward:0.10000000149011612 last n mean rew.0.026300000436604024,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:520 steps:31 episode_reward:0.10000000149011612 last n mean rew.0.027300000432878734,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:530 steps:21 episode_reward:0.0 last n mean rew.0.026300000417977573,individual episode rew.[-0.01  0.  ]\n",
      "Episode:540 steps:17 episode_reward:0.0 last n mean rew.0.027200000435113905,individual episode rew.[ 0.   -0.01]\n",
      "Episode:550 steps:14 episode_reward:0.0 last n mean rew.0.02610000042244792,individual episode rew.[-0.01  0.  ]\n",
      "Episode:560 steps:15 episode_reward:0.0 last n mean rew.0.0222000003606081,individual episode rew.[-0.01  0.  ]\n",
      "Episode:570 steps:14 episode_reward:0.0 last n mean rew.0.020000000335276127,individual episode rew.[ 0.   -0.01]\n",
      "Episode:580 steps:14 episode_reward:0.0 last n mean rew.0.019000000320374966,individual episode rew.[ 0.   -0.01]\n",
      "Episode:590 steps:14 episode_reward:0.0 last n mean rew.0.021800000369548798,individual episode rew.[ 0.   -0.01]\n",
      "Episode:600 steps:15 episode_reward:0.0 last n mean rew.0.02280000038444996,individual episode rew.[ 0.   -0.01]\n",
      "Episode:610 steps:14 episode_reward:0.0 last n mean rew.0.020000000335276127,individual episode rew.[-0.01  0.  ]\n",
      "Episode:620 steps:14 episode_reward:0.0 last n mean rew.0.015200000256299973,individual episode rew.[ 0.   -0.01]\n",
      "Episode:630 steps:14 episode_reward:0.0 last n mean rew.0.01320000022649765,individual episode rew.[-0.01  0.  ]\n",
      "Episode:640 steps:29 episode_reward:0.10000000149011612 last n mean rew.0.013300000224262476,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:650 steps:14 episode_reward:0.0 last n mean rew.0.015300000254064798,individual episode rew.[ 0.   -0.01]\n",
      "Episode:660 steps:14 episode_reward:0.0 last n mean rew.0.017200000286102295,individual episode rew.[ 0.   -0.01]\n",
      "Episode:670 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.017200000286102295,individual episode rew.[0.   0.09]\n",
      "Episode:680 steps:15 episode_reward:0.0 last n mean rew.0.017200000286102295,individual episode rew.[-0.01  0.  ]\n",
      "Episode:690 steps:14 episode_reward:0.0 last n mean rew.0.014400000236928463,individual episode rew.[-0.01  0.  ]\n",
      "Episode:700 steps:14 episode_reward:0.0 last n mean rew.0.010500000175088644,individual episode rew.[ 0.   -0.01]\n",
      "Episode:710 steps:14 episode_reward:0.0 last n mean rew.0.010400000177323818,individual episode rew.[ 0.   -0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:720 steps:14 episode_reward:0.0 last n mean rew.0.010400000177323818,individual episode rew.[ 0.   -0.01]\n",
      "Episode:730 steps:15 episode_reward:0.0 last n mean rew.0.011300000194460154,individual episode rew.[ 0.   -0.01]\n",
      "Episode:740 steps:14 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[ 0.   -0.01]\n",
      "Episode:750 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:760 steps:33 episode_reward:0.09000000171363354 last n mean rew.0.009300000164657832,individual episode rew.[0.   0.09]\n",
      "Episode:770 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:780 steps:14 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[-0.01  0.  ]\n",
      "Episode:790 steps:15 episode_reward:0.0 last n mean rew.0.007600000128149986,individual episode rew.[-0.01  0.  ]\n",
      "Episode:800 steps:15 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[ 0.   -0.01]\n",
      "Episode:810 steps:14 episode_reward:0.0 last n mean rew.0.007600000128149986,individual episode rew.[-0.01  0.  ]\n",
      "Episode:820 steps:15 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[-0.01  0.  ]\n",
      "Episode:830 steps:14 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[ 0.   -0.01]\n",
      "Episode:840 steps:14 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:850 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:860 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:870 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:880 steps:14 episode_reward:0.0 last n mean rew.0.007300000153481961,individual episode rew.[ 0.   -0.01]\n",
      "Episode:890 steps:14 episode_reward:0.0 last n mean rew.0.007200000155717134,individual episode rew.[ 0.   -0.01]\n",
      "Episode:900 steps:14 episode_reward:0.0 last n mean rew.0.006300000138580799,individual episode rew.[-0.01  0.  ]\n",
      "Episode:910 steps:14 episode_reward:0.0 last n mean rew.0.006300000138580799,individual episode rew.[-0.01  0.  ]\n",
      "Episode:920 steps:14 episode_reward:0.0 last n mean rew.0.006300000138580799,individual episode rew.[ 0.   -0.01]\n",
      "Episode:930 steps:14 episode_reward:0.0 last n mean rew.0.003600000087171793,individual episode rew.[ 0.   -0.01]\n",
      "Episode:940 steps:14 episode_reward:0.0 last n mean rew.0.004500000104308129,individual episode rew.[-0.01  0.  ]\n",
      "Episode:950 steps:14 episode_reward:0.0 last n mean rew.0.003600000087171793,individual episode rew.[-0.01  0.  ]\n",
      "Episode:960 steps:15 episode_reward:0.0 last n mean rew.0.004500000104308129,individual episode rew.[ 0.   -0.01]\n",
      "Episode:970 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.005400000121444464,individual episode rew.[0.   0.09]\n",
      "Episode:980 steps:15 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[ 0.   -0.01]\n",
      "Episode:990 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1000 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1010 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1020 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1030 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1040 steps:15 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1050 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1060 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.005400000102818012,individual episode rew.[0.   0.09]\n",
      "Episode:1070 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1080 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1090 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1100 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1110 steps:29 episode_reward:0.09000000171363354 last n mean rew.0.007200000137090683,individual episode rew.[0.   0.09]\n",
      "Episode:1120 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1130 steps:15 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1140 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1150 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1160 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1170 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1180 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1190 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1200 steps:14 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1210 steps:14 episode_reward:0.0 last n mean rew.0.002700000051409006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1220 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.0036000000685453416,individual episode rew.[0.   0.09]\n",
      "Episode:1230 steps:14 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1240 steps:14 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1250 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1260 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1270 steps:15 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1280 steps:15 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1290 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1300 steps:14 episode_reward:0.0 last n mean rew.0.007400000151246786,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1310 steps:14 episode_reward:0.0 last n mean rew.0.008400000166147947,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1320 steps:14 episode_reward:0.0 last n mean rew.0.008400000166147947,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1330 steps:14 episode_reward:0.0 last n mean rew.0.008400000166147947,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1340 steps:14 episode_reward:0.0 last n mean rew.0.009400000181049108,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1350 steps:14 episode_reward:0.0 last n mean rew.0.007500000149011612,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1360 steps:14 episode_reward:0.0 last n mean rew.0.008500000163912773,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1370 steps:15 episode_reward:0.0 last n mean rew.0.008500000163912773,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1380 steps:15 episode_reward:0.0 last n mean rew.0.009400000181049108,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1390 steps:30 episode_reward:0.10000000149011612 last n mean rew.0.010300000198185444,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:1400 steps:15 episode_reward:0.0 last n mean rew.0.009400000162422657,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1410 steps:15 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1420 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1430 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1440 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1450 steps:15 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1460 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.006400000117719173,individual episode rew.[0.   0.09]\n",
      "Episode:1470 steps:13 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1480 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1490 steps:16 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1500 steps:14 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1510 steps:14 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1520 steps:14 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1530 steps:14 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1540 steps:33 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[-0.02  0.  ]\n",
      "Episode:1550 steps:14 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1560 steps:14 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1570 steps:14 episode_reward:0.0 last n mean rew.0.012300000209361315,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1580 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.01320000022649765,individual episode rew.[0.   0.09]\n",
      "Episode:1590 steps:14 episode_reward:0.0 last n mean rew.0.015200000256299973,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1600 steps:16 episode_reward:0.0 last n mean rew.0.015100000258535147,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1610 steps:30 episode_reward:0.10000000149011612 last n mean rew.0.016200000271201134,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:1620 steps:30 episode_reward:0.10000000149011612 last n mean rew.0.019200000315904617,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:1630 steps:14 episode_reward:0.0 last n mean rew.0.021000000350177288,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1640 steps:14 episode_reward:0.0 last n mean rew.0.021900000367313623,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1650 steps:14 episode_reward:0.0 last n mean rew.0.023900000397115945,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1660 steps:15 episode_reward:0.0 last n mean rew.0.02300000037997961,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1670 steps:14 episode_reward:0.0 last n mean rew.0.020100000333040952,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1680 steps:13 episode_reward:0.0 last n mean rew.0.020000000335276127,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1690 steps:15 episode_reward:0.0 last n mean rew.0.01710000028833747,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1700 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.016100000273436308,individual episode rew.[0.   0.09]\n",
      "Episode:1710 steps:14 episode_reward:0.0 last n mean rew.0.015100000258535147,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1720 steps:14 episode_reward:0.0 last n mean rew.0.014100000243633986,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1730 steps:15 episode_reward:0.0 last n mean rew.0.015200000256299973,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1740 steps:14 episode_reward:0.0 last n mean rew.0.015300000254064798,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1750 steps:14 episode_reward:0.0 last n mean rew.0.014300000239163637,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1760 steps:15 episode_reward:0.0 last n mean rew.0.015300000254064798,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1770 steps:46 episode_reward:0.09000000171363354 last n mean rew.0.018200000301003456,individual episode rew.[0.   0.09]\n",
      "Episode:1780 steps:14 episode_reward:0.0 last n mean rew.0.01830000029876828,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1790 steps:15 episode_reward:0.0 last n mean rew.0.016400000266730785,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1800 steps:14 episode_reward:0.0 last n mean rew.0.01750000027939677,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1810 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.019300000313669442,individual episode rew.[0.   0.09]\n",
      "Episode:1820 steps:14 episode_reward:0.0 last n mean rew.0.01730000028386712,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1830 steps:14 episode_reward:0.0 last n mean rew.0.015300000254064798,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1840 steps:16 episode_reward:0.0 last n mean rew.0.01710000028833747,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1850 steps:30 episode_reward:0.10000000149011612 last n mean rew.0.0199000003375113,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:1860 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.02070000035688281,individual episode rew.[0.   0.09]\n",
      "Episode:1870 steps:15 episode_reward:0.0 last n mean rew.0.02070000035688281,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1880 steps:68 episode_reward:0.20000000298023224 last n mean rew.0.023700000401586294,individual episode rew.[0.2  0.09]\n",
      "Episode:1890 steps:25 episode_reward:0.0 last n mean rew.0.024700000416487455,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1900 steps:14 episode_reward:0.0 last n mean rew.0.02280000038444996,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1910 steps:29 episode_reward:0.10000000149011612 last n mean rew.0.02400000039488077,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:1920 steps:33 episode_reward:0.10000000149011612 last n mean rew.0.02990000048652291,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:1930 steps:14 episode_reward:0.0 last n mean rew.0.0310000004991889,individual episode rew.[ 0.   -0.01]\n",
      "Episode:1940 steps:13 episode_reward:0.0 last n mean rew.0.02920000046491623,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1950 steps:29 episode_reward:0.10000000149011612 last n mean rew.0.026500000413507224,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:1960 steps:14 episode_reward:0.0 last n mean rew.0.025700000394135714,individual episode rew.[-0.01  0.  ]\n",
      "Episode:1970 steps:29 episode_reward:0.09000000171363354 last n mean rew.0.02560000039637089,individual episode rew.[0.   0.09]\n",
      "Episode:1980 steps:29 episode_reward:0.09000000171363354 last n mean rew.0.023600000366568567,individual episode rew.[0.   0.09]\n",
      "Episode:1990 steps:16 episode_reward:0.0 last n mean rew.0.024500000383704902,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2000 steps:14 episode_reward:0.0 last n mean rew.0.025400000400841238,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2010 steps:29 episode_reward:0.10000000149011612 last n mean rew.0.024300000388175248,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:2020 steps:14 episode_reward:0.0 last n mean rew.0.018400000296533107,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2030 steps:32 episode_reward:0.10000000149011612 last n mean rew.0.019300000313669442,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:2040 steps:15 episode_reward:0.0 last n mean rew.0.01830000029876828,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2050 steps:27 episode_reward:0.10000000149011612 last n mean rew.0.018200000301003456,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:2060 steps:29 episode_reward:0.10000000149011612 last n mean rew.0.020200000330805778,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:2070 steps:15 episode_reward:0.0 last n mean rew.0.019300000332295893,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2080 steps:31 episode_reward:0.10000000149011612 last n mean rew.0.019200000334531068,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:2090 steps:14 episode_reward:0.0 last n mean rew.0.022100000381469725,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2100 steps:14 episode_reward:0.0 last n mean rew.0.023200000394135712,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2110 steps:14 episode_reward:0.0 last n mean rew.0.02120000036433339,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:2120 steps:33 episode_reward:0.10000000149011612 last n mean rew.0.02220000037923455,individual episode rew.[ 0.1  -0.01]\n",
      "Episode:2130 steps:14 episode_reward:0.0 last n mean rew.0.02020000034943223,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2140 steps:14 episode_reward:0.0 last n mean rew.0.02020000034943223,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2150 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.020100000351667403,individual episode rew.[0.   0.09]\n",
      "Episode:2160 steps:15 episode_reward:0.0 last n mean rew.0.018000000324100256,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2170 steps:14 episode_reward:0.0 last n mean rew.0.019000000320374966,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2180 steps:15 episode_reward:0.0 last n mean rew.0.01710000028833747,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2190 steps:14 episode_reward:0.0 last n mean rew.0.013300000224262476,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2200 steps:14 episode_reward:0.0 last n mean rew.0.012100000213831664,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2210 steps:14 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2220 steps:14 episode_reward:0.0 last n mean rew.0.011100000198930503,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2230 steps:15 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2240 steps:14 episode_reward:0.0 last n mean rew.0.012000000216066837,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2250 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2260 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2270 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2280 steps:15 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2290 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2300 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.006400000117719173,individual episode rew.[0.   0.09]\n",
      "Episode:2310 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2320 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.009100000169128179,individual episode rew.[0.   0.09]\n",
      "Episode:2330 steps:14 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2340 steps:15 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2350 steps:29 episode_reward:0.09000000171363354 last n mean rew.0.01090000020340085,individual episode rew.[0.   0.09]\n",
      "Episode:2360 steps:14 episode_reward:0.0 last n mean rew.0.012700000237673521,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2370 steps:14 episode_reward:0.0 last n mean rew.0.012700000237673521,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2380 steps:14 episode_reward:0.0 last n mean rew.0.01170000022277236,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2390 steps:25 episode_reward:0.0 last n mean rew.0.013500000257045031,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2400 steps:14 episode_reward:0.0 last n mean rew.0.013500000257045031,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2410 steps:14 episode_reward:0.0 last n mean rew.0.013500000257045031,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2420 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2430 steps:14 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2440 steps:14 episode_reward:0.0 last n mean rew.0.012700000237673521,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2450 steps:14 episode_reward:0.0 last n mean rew.0.012700000237673521,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2460 steps:15 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2470 steps:14 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2480 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2490 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2500 steps:15 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2510 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2520 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2530 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2540 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2550 steps:15 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2560 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2570 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2580 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2590 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.008200000151991843,individual episode rew.[0.   0.09]\n",
      "Episode:2600 steps:15 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2610 steps:15 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2620 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2630 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2640 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2650 steps:16 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2660 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2670 steps:15 episode_reward:0.0 last n mean rew.0.011100000198930503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2680 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2690 steps:18 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2700 steps:15 episode_reward:0.0 last n mean rew.0.013400000222027302,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2710 steps:14 episode_reward:0.0 last n mean rew.0.01910000031813979,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2720 steps:14 episode_reward:0.0 last n mean rew.0.020100000333040952,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2730 steps:61 episode_reward:0.10000000149011612 last n mean rew.0.024900000412017106,individual episode rew.[0.1  0.09]\n",
      "Episode:2740 steps:15 episode_reward:0.0 last n mean rew.0.026700000446289777,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2750 steps:14 episode_reward:0.0 last n mean rew.0.026700000446289777,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2760 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.026800000444054603,individual episode rew.[0.   0.09]\n",
      "Episode:2770 steps:14 episode_reward:0.0 last n mean rew.0.02770000046119094,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2780 steps:15 episode_reward:0.0 last n mean rew.0.02970000050961971,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2790 steps:15 episode_reward:0.0 last n mean rew.0.029600000511854886,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2800 steps:15 episode_reward:0.0 last n mean rew.0.025600000452250242,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2810 steps:14 episode_reward:0.0 last n mean rew.0.020800000373274088,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2820 steps:14 episode_reward:0.0 last n mean rew.0.020700000375509262,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:2830 steps:17 episode_reward:0.0 last n mean rew.0.016800000313669444,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2840 steps:14 episode_reward:0.0 last n mean rew.0.015900000296533108,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2850 steps:15 episode_reward:0.0 last n mean rew.0.014900000281631947,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2860 steps:15 episode_reward:0.0 last n mean rew.0.015700000301003457,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2870 steps:20 episode_reward:0.0 last n mean rew.0.014600000288337469,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2880 steps:15 episode_reward:0.0 last n mean rew.0.012600000239908695,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2890 steps:14 episode_reward:0.0 last n mean rew.0.013700000252574682,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2900 steps:15 episode_reward:0.0 last n mean rew.0.018600000329315663,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2910 steps:15 episode_reward:0.0 last n mean rew.0.018600000329315663,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2920 steps:14 episode_reward:0.0 last n mean rew.0.016800000295042992,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2930 steps:14 episode_reward:0.0 last n mean rew.0.017700000312179328,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2940 steps:14 episode_reward:0.0 last n mean rew.0.015900000277906657,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2950 steps:15 episode_reward:0.0 last n mean rew.0.016800000295042992,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2960 steps:14 episode_reward:0.0 last n mean rew.0.014100000243633986,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2970 steps:30 episode_reward:0.0 last n mean rew.0.013400000222027302,individual episode rew.[ 0.   -0.01]\n",
      "Episode:2980 steps:14 episode_reward:0.0 last n mean rew.0.012500000204890966,individual episode rew.[-0.01  0.  ]\n",
      "Episode:2990 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.01140000019222498,individual episode rew.[0.   0.09]\n",
      "Episode:3000 steps:30 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3010 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3020 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3030 steps:18 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3040 steps:14 episode_reward:0.0 last n mean rew.0.007600000128149986,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3050 steps:15 episode_reward:0.0 last n mean rew.0.007600000128149986,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3060 steps:14 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3070 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3080 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.007400000132620334,individual episode rew.[0.   0.09]\n",
      "Episode:3090 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3100 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3110 steps:15 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3120 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3130 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3140 steps:33 episode_reward:0.10000000149011612 last n mean rew.0.01010000018402934,individual episode rew.[-0.02  0.1 ]\n",
      "Episode:3150 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3160 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3170 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.011900000218302011,individual episode rew.[0.   0.09]\n",
      "Episode:3180 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3190 steps:17 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3200 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3210 steps:15 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3220 steps:33 episode_reward:0.10000000149011612 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:3230 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3240 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3250 steps:15 episode_reward:0.0 last n mean rew.0.011800000220537185,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3260 steps:14 episode_reward:0.0 last n mean rew.0.012700000237673521,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3270 steps:14 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3280 steps:15 episode_reward:0.0 last n mean rew.0.014600000269711018,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3290 steps:21 episode_reward:0.0 last n mean rew.0.016500000301748516,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3300 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.015600000284612179,individual episode rew.[0.   0.09]\n",
      "Episode:3310 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.016500000301748516,individual episode rew.[0.   0.09]\n",
      "Episode:3320 steps:14 episode_reward:0.0 last n mean rew.0.017300000321120022,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3330 steps:15 episode_reward:0.0 last n mean rew.0.01920000035315752,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3340 steps:14 episode_reward:0.0 last n mean rew.0.01920000035315752,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3350 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.01920000035315752,individual episode rew.[0.   0.09]\n",
      "Episode:3360 steps:15 episode_reward:0.0 last n mean rew.0.01920000035315752,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3370 steps:14 episode_reward:0.0 last n mean rew.0.02100000038743019,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3380 steps:14 episode_reward:0.0 last n mean rew.0.01920000035315752,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3390 steps:14 episode_reward:0.0 last n mean rew.0.018200000338256358,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3400 steps:14 episode_reward:0.0 last n mean rew.0.016400000303983687,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3410 steps:14 episode_reward:0.0 last n mean rew.0.016400000303983687,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3420 steps:15 episode_reward:0.0 last n mean rew.0.014600000269711018,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3430 steps:14 episode_reward:0.0 last n mean rew.0.011800000220537185,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3440 steps:14 episode_reward:0.0 last n mean rew.0.011800000220537185,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3450 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3460 steps:17 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3470 steps:33 episode_reward:0.10000000149011612 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:3480 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3490 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3500 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3510 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3520 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.006400000117719173,individual episode rew.[0.   0.09]\n",
      "Episode:3530 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:3540 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3550 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.007300000134855509,individual episode rew.[0.   0.09]\n",
      "Episode:3560 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3570 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.007200000137090683,individual episode rew.[0.   0.09]\n",
      "Episode:3580 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3590 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3600 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3610 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3620 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3630 steps:40 episode_reward:0.10000000149011612 last n mean rew.0.012100000213831664,individual episode rew.[0.1  0.09]\n",
      "Episode:3640 steps:15 episode_reward:0.0 last n mean rew.0.013900000248104333,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3650 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.013900000248104333,individual episode rew.[0.   0.09]\n",
      "Episode:3660 steps:13 episode_reward:0.0 last n mean rew.0.015700000282377006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3670 steps:14 episode_reward:0.0 last n mean rew.0.013900000248104333,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3680 steps:14 episode_reward:0.0 last n mean rew.0.013000000230968,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3690 steps:15 episode_reward:0.0 last n mean rew.0.013800000250339508,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3700 steps:14 episode_reward:0.0 last n mean rew.0.012800000235438347,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3710 steps:14 episode_reward:0.0 last n mean rew.0.013700000252574682,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3720 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.014600000269711018,individual episode rew.[0.   0.09]\n",
      "Episode:3730 steps:14 episode_reward:0.0 last n mean rew.0.012600000239908695,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3740 steps:14 episode_reward:0.0 last n mean rew.0.012600000239908695,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3750 steps:15 episode_reward:0.0 last n mean rew.0.013500000257045031,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3760 steps:14 episode_reward:0.0 last n mean rew.0.014400000274181366,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3770 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.015300000291317702,individual episode rew.[0.   0.09]\n",
      "Episode:3780 steps:14 episode_reward:0.0 last n mean rew.0.015300000291317702,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3790 steps:15 episode_reward:0.0 last n mean rew.0.012600000239908695,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3800 steps:14 episode_reward:0.0 last n mean rew.0.012600000239908695,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3810 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3820 steps:15 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3830 steps:15 episode_reward:0.0 last n mean rew.0.009900000188499689,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3840 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.008100000154227018,individual episode rew.[0.   0.09]\n",
      "Episode:3850 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3860 steps:14 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3870 steps:14 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3880 steps:14 episode_reward:0.0 last n mean rew.0.00550000011920929,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3890 steps:15 episode_reward:0.0 last n mean rew.0.006400000136345625,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3900 steps:14 episode_reward:0.0 last n mean rew.0.00550000011920929,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3910 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.007400000151246786,individual episode rew.[0.   0.09]\n",
      "Episode:3920 steps:15 episode_reward:0.0 last n mean rew.0.009200000185519457,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3930 steps:14 episode_reward:0.0 last n mean rew.0.008300000168383122,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3940 steps:14 episode_reward:0.0 last n mean rew.0.008300000168383122,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3950 steps:14 episode_reward:0.0 last n mean rew.0.007300000153481961,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3960 steps:14 episode_reward:0.0 last n mean rew.0.008200000170618296,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3970 steps:15 episode_reward:0.0 last n mean rew.0.009200000185519457,individual episode rew.[-0.01  0.  ]\n",
      "Episode:3980 steps:16 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:3990 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.00830000014975667,individual episode rew.[0.   0.09]\n",
      "Episode:4000 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4010 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4020 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4030 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4040 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4050 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.009200000166893006,individual episode rew.[0.   0.09]\n",
      "Episode:4060 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4070 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4080 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4090 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4100 steps:15 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4110 steps:15 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4120 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4130 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4140 steps:13 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4150 steps:14 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4160 steps:16 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4170 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4180 steps:14 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4190 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4200 steps:32 episode_reward:0.10000000149011612 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:4210 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4220 steps:14 episode_reward:0.0 last n mean rew.0.011100000198930503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4230 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:4240 steps:14 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4250 steps:15 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4260 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4270 steps:14 episode_reward:0.0 last n mean rew.0.008400000166147947,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4280 steps:14 episode_reward:0.0 last n mean rew.0.008400000166147947,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4290 steps:14 episode_reward:0.0 last n mean rew.0.011100000217556954,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4300 steps:14 episode_reward:0.0 last n mean rew.0.011000000219792128,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4310 steps:14 episode_reward:0.0 last n mean rew.0.011000000219792128,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4320 steps:14 episode_reward:0.0 last n mean rew.0.009100000187754632,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4330 steps:14 episode_reward:0.0 last n mean rew.0.01200000023469329,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4340 steps:14 episode_reward:0.0 last n mean rew.0.01200000023469329,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4350 steps:14 episode_reward:0.0 last n mean rew.0.012900000251829625,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4360 steps:15 episode_reward:0.0 last n mean rew.0.012900000251829625,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4370 steps:14 episode_reward:0.0 last n mean rew.0.012900000233203172,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4380 steps:14 episode_reward:0.0 last n mean rew.0.012000000216066837,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4390 steps:15 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4400 steps:15 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4410 steps:13 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4420 steps:14 episode_reward:0.0 last n mean rew.0.009400000162422657,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4430 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4440 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4450 steps:14 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4460 steps:15 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4470 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4480 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4490 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4500 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4510 steps:14 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4520 steps:15 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4530 steps:15 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4540 steps:14 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4550 steps:14 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4560 steps:15 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4570 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4580 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.010200000181794167,individual episode rew.[0.   0.09]\n",
      "Episode:4590 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4600 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4610 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4620 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4630 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4640 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4650 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4660 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4670 steps:15 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4680 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4690 steps:15 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4700 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4710 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4720 steps:19 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4730 steps:34 episode_reward:0.10000000149011612 last n mean rew.0.005600000098347664,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:4740 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4750 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4760 steps:15 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4770 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4780 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.007400000132620334,individual episode rew.[0.   0.09]\n",
      "Episode:4790 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4800 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4810 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4820 steps:15 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4830 steps:14 episode_reward:0.0 last n mean rew.0.01170000022277236,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4840 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4850 steps:16 episode_reward:0.0 last n mean rew.0.012600000239908695,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4860 steps:14 episode_reward:0.0 last n mean rew.0.012700000237673521,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4870 steps:14 episode_reward:0.0 last n mean rew.0.011800000220537185,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4880 steps:14 episode_reward:0.0 last n mean rew.0.011800000220537185,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4890 steps:15 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4900 steps:14 episode_reward:0.0 last n mean rew.0.012000000216066837,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4910 steps:14 episode_reward:0.0 last n mean rew.0.012100000213831664,individual episode rew.[ 0.   -0.01]\n",
      "Episode:4920 steps:14 episode_reward:0.0 last n mean rew.0.009400000162422657,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4930 steps:15 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4940 steps:14 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:4950 steps:14 episode_reward:0.0 last n mean rew.0.006700000111013651,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4960 steps:15 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4970 steps:14 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4980 steps:15 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[-0.01  0.  ]\n",
      "Episode:4990 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5000 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5010 steps:15 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5020 steps:15 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5030 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5040 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5050 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5060 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5070 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5080 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5090 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5100 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5110 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5120 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5130 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5140 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5150 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5160 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5170 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5180 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5190 steps:15 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5200 steps:14 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5210 steps:14 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5220 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5230 steps:14 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5240 steps:17 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5250 steps:33 episode_reward:0.09000000171363354 last n mean rew.0.006400000117719173,individual episode rew.[0.   0.09]\n",
      "Episode:5260 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5270 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5280 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5290 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5300 steps:14 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5310 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5320 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5330 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.006400000117719173,individual episode rew.[0.   0.09]\n",
      "Episode:5340 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5350 steps:14 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5360 steps:29 episode_reward:0.09000000171363354 last n mean rew.0.005500000100582838,individual episode rew.[0.   0.09]\n",
      "Episode:5370 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.008200000151991843,individual episode rew.[0.   0.09]\n",
      "Episode:5380 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5390 steps:15 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5400 steps:14 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5410 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5420 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5430 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5440 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5450 steps:14 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5460 steps:14 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5470 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5480 steps:17 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5490 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5500 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5510 steps:14 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5520 steps:16 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5530 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5540 steps:14 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5550 steps:14 episode_reward:0.0 last n mean rew.0.009900000188499689,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5560 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5570 steps:15 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5580 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5590 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5600 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5610 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5620 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5630 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5640 steps:15 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5650 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:5660 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5670 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5680 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5690 steps:14 episode_reward:0.0 last n mean rew.0.01170000022277236,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5700 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5710 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5720 steps:30 episode_reward:0.10000000149011612 last n mean rew.0.011800000220537185,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:5730 steps:14 episode_reward:0.0 last n mean rew.0.013700000252574682,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5740 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.013800000250339508,individual episode rew.[0.   0.09]\n",
      "Episode:5750 steps:15 episode_reward:0.0 last n mean rew.0.014700000267475843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5760 steps:14 episode_reward:0.0 last n mean rew.0.013800000250339508,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5770 steps:14 episode_reward:0.0 last n mean rew.0.012000000216066837,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5780 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5790 steps:15 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5800 steps:15 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5810 steps:21 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5820 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5830 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5840 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5850 steps:14 episode_reward:0.0 last n mean rew.0.002700000051409006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5860 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5870 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5880 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5890 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5900 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5910 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5920 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:5930 steps:16 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5940 steps:25 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5950 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.010300000179558993,individual episode rew.[0.   0.09]\n",
      "Episode:5960 steps:33 episode_reward:0.10000000149011612 last n mean rew.0.010400000177323818,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:5970 steps:14 episode_reward:0.0 last n mean rew.0.010400000177323818,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5980 steps:14 episode_reward:0.0 last n mean rew.0.011300000194460154,individual episode rew.[ 0.   -0.01]\n",
      "Episode:5990 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.013100000228732825,individual episode rew.[0.   0.09]\n",
      "Episode:6000 steps:14 episode_reward:0.0 last n mean rew.0.01220000021159649,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6010 steps:14 episode_reward:0.0 last n mean rew.0.014000000264495611,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6020 steps:14 episode_reward:0.0 last n mean rew.0.01200000023469329,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6030 steps:14 episode_reward:0.0 last n mean rew.0.01200000023469329,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6040 steps:14 episode_reward:0.0 last n mean rew.0.011000000219792128,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6050 steps:14 episode_reward:0.0 last n mean rew.0.008200000170618296,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6060 steps:14 episode_reward:0.0 last n mean rew.0.006300000138580799,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6070 steps:15 episode_reward:0.0 last n mean rew.0.005400000121444464,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6080 steps:14 episode_reward:0.0 last n mean rew.0.006400000136345625,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6090 steps:14 episode_reward:0.0 last n mean rew.0.006500000134110451,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6100 steps:14 episode_reward:0.0 last n mean rew.0.007400000151246786,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6110 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6120 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6130 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6140 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6150 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6160 steps:15 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6170 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6180 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6190 steps:14 episode_reward:0.0 last n mean rew.0.005600000098347664,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6200 steps:14 episode_reward:0.0 last n mean rew.0.004700000081211329,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6210 steps:14 episode_reward:0.0 last n mean rew.0.0029000000469386576,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6220 steps:14 episode_reward:0.0 last n mean rew.0.0019000000320374966,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6230 steps:14 episode_reward:0.0 last n mean rew.0.0028000000678002836,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6240 steps:14 episode_reward:0.0 last n mean rew.0.0028000000678002836,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6250 steps:15 episode_reward:0.0 last n mean rew.0.0037000000849366187,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6260 steps:14 episode_reward:0.0 last n mean rew.0.004600000102072954,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6270 steps:14 episode_reward:0.0 last n mean rew.0.007300000153481961,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6280 steps:14 episode_reward:0.0 last n mean rew.0.007300000153481961,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6290 steps:14 episode_reward:0.0 last n mean rew.0.006300000138580799,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6300 steps:15 episode_reward:0.0 last n mean rew.0.006300000138580799,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6310 steps:16 episode_reward:0.0 last n mean rew.0.008200000170618296,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6320 steps:14 episode_reward:0.0 last n mean rew.0.008200000170618296,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6330 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6340 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6350 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6360 steps:15 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:6370 steps:14 episode_reward:0.0 last n mean rew.0.004700000081211329,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6380 steps:14 episode_reward:0.0 last n mean rew.0.003800000064074993,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6390 steps:14 episode_reward:0.0 last n mean rew.0.003800000064074993,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6400 steps:14 episode_reward:0.0 last n mean rew.0.004700000081211329,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6410 steps:14 episode_reward:0.0 last n mean rew.0.002800000049173832,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6420 steps:14 episode_reward:0.0 last n mean rew.0.002800000049173832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6430 steps:15 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6440 steps:14 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6450 steps:15 episode_reward:0.0 last n mean rew.0.007600000128149986,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6460 steps:14 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6470 steps:14 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6480 steps:15 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6490 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.008400000147521496,individual episode rew.[0.   0.09]\n",
      "Episode:6500 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6510 steps:14 episode_reward:0.0 last n mean rew.0.011100000198930503,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6520 steps:14 episode_reward:0.0 last n mean rew.0.011100000198930503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6530 steps:14 episode_reward:0.0 last n mean rew.0.011100000198930503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6540 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6550 steps:14 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6560 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6570 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6580 steps:14 episode_reward:0.0 last n mean rew.0.010800000205636024,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6590 steps:14 episode_reward:0.0 last n mean rew.0.009900000188499689,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6600 steps:15 episode_reward:0.0 last n mean rew.0.009900000188499689,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6610 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6620 steps:15 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6630 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6640 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6650 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6660 steps:29 episode_reward:0.09000000171363354 last n mean rew.0.009100000169128179,individual episode rew.[0.   0.09]\n",
      "Episode:6670 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6680 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6690 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6700 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6710 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6720 steps:13 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6730 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6740 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6750 steps:14 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6760 steps:14 episode_reward:0.0 last n mean rew.0.002800000049173832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6770 steps:14 episode_reward:0.0 last n mean rew.0.0009000000171363354,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6780 steps:14 episode_reward:0.0 last n mean rew.0.0009000000171363354,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6790 steps:33 episode_reward:0.10000000149011612 last n mean rew.0.0019000000320374966,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:6800 steps:14 episode_reward:0.0 last n mean rew.0.0019000000320374966,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6810 steps:14 episode_reward:0.0 last n mean rew.0.002800000049173832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6820 steps:15 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6830 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.007400000132620334,individual episode rew.[0.   0.09]\n",
      "Episode:6840 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6850 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6860 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6870 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.009200000166893006,individual episode rew.[0.   0.09]\n",
      "Episode:6880 steps:15 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6890 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6900 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6910 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6920 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6930 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6940 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6950 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6960 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6970 steps:15 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[ 0.   -0.01]\n",
      "Episode:6980 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[-0.01  0.  ]\n",
      "Episode:6990 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7000 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7010 steps:16 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7020 steps:25 episode_reward:0.10000000149011612 last n mean rew.0.009300000164657832,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:7030 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7040 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.008400000147521496,individual episode rew.[0.   0.09]\n",
      "Episode:7050 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.008400000147521496,individual episode rew.[0.   0.09]\n",
      "Episode:7060 steps:14 episode_reward:0.0 last n mean rew.0.009400000162422657,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7070 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.009400000162422657,individual episode rew.[0.   0.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:7080 steps:14 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7090 steps:14 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7100 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7110 steps:15 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7120 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.008200000151991843,individual episode rew.[0.   0.09]\n",
      "Episode:7130 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7140 steps:15 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7150 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7160 steps:15 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7170 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7180 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7190 steps:14 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7200 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.014700000267475843,individual episode rew.[0.   0.09]\n",
      "Episode:7210 steps:14 episode_reward:0.0 last n mean rew.0.014700000267475843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7220 steps:14 episode_reward:0.0 last n mean rew.0.013900000248104333,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7230 steps:14 episode_reward:0.0 last n mean rew.0.013900000248104333,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7240 steps:14 episode_reward:0.0 last n mean rew.0.013000000230968,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7250 steps:13 episode_reward:0.0 last n mean rew.0.01220000021159649,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7260 steps:14 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7270 steps:14 episode_reward:0.0 last n mean rew.0.012100000213831664,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7280 steps:14 episode_reward:0.0 last n mean rew.0.011200000196695328,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7290 steps:15 episode_reward:0.0 last n mean rew.0.009400000162422657,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7300 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7310 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7320 steps:14 episode_reward:0.0 last n mean rew.0.005600000098347664,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7330 steps:15 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7340 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7350 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7360 steps:16 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7370 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7380 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7390 steps:14 episode_reward:0.0 last n mean rew.0.008200000170618296,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7400 steps:15 episode_reward:0.0 last n mean rew.0.006400000136345625,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7410 steps:15 episode_reward:0.0 last n mean rew.0.006400000136345625,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7420 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.008200000170618296,individual episode rew.[0.   0.09]\n",
      "Episode:7430 steps:14 episode_reward:0.0 last n mean rew.0.007300000153481961,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7440 steps:14 episode_reward:0.0 last n mean rew.0.006400000136345625,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7450 steps:14 episode_reward:0.0 last n mean rew.0.006400000136345625,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7460 steps:14 episode_reward:0.0 last n mean rew.0.007300000153481961,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7470 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.009200000185519457,individual episode rew.[0.   0.09]\n",
      "Episode:7480 steps:14 episode_reward:0.0 last n mean rew.0.011100000217556954,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7490 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7500 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.011100000198930503,individual episode rew.[0.   0.09]\n",
      "Episode:7510 steps:14 episode_reward:0.0 last n mean rew.0.012000000216066837,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7520 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7530 steps:14 episode_reward:0.0 last n mean rew.0.013900000248104333,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7540 steps:15 episode_reward:0.0 last n mean rew.0.014900000263005496,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7550 steps:15 episode_reward:0.0 last n mean rew.0.013900000248104333,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7560 steps:15 episode_reward:0.0 last n mean rew.0.013000000230968,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7570 steps:15 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7580 steps:15 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7590 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7600 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7610 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7620 steps:14 episode_reward:0.0 last n mean rew.0.011100000198930503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7630 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7640 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7650 steps:15 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7660 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7670 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7680 steps:15 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7690 steps:15 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7700 steps:14 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7710 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.009300000164657832,individual episode rew.[0.   0.09]\n",
      "Episode:7720 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7730 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7740 steps:15 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7750 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7760 steps:15 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7770 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7780 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[ 0.   -0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:7790 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7800 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7810 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7820 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7830 steps:13 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7840 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7850 steps:14 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7860 steps:14 episode_reward:0.0 last n mean rew.0.009300000164657832,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7870 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7880 steps:15 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7890 steps:15 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7900 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7910 steps:15 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7920 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7930 steps:14 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7940 steps:15 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7950 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7960 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:7970 steps:13 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7980 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:7990 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8000 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.012000000216066837,individual episode rew.[0.   0.09]\n",
      "Episode:8010 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8020 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8030 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8040 steps:14 episode_reward:0.0 last n mean rew.0.010200000181794167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8050 steps:14 episode_reward:0.0 last n mean rew.0.009400000162422657,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8060 steps:15 episode_reward:0.0 last n mean rew.0.008500000145286322,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8070 steps:15 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8080 steps:14 episode_reward:0.0 last n mean rew.0.004700000081211329,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8090 steps:14 episode_reward:0.0 last n mean rew.0.005600000098347664,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8100 steps:14 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8110 steps:14 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8120 steps:14 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8130 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8140 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8150 steps:14 episode_reward:0.0 last n mean rew.0.006400000117719173,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8160 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.008200000151991843,individual episode rew.[0.   0.09]\n",
      "Episode:8170 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8180 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8190 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8200 steps:15 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8210 steps:14 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8220 steps:15 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8230 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8240 steps:14 episode_reward:0.0 last n mean rew.0.009000000171363353,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8250 steps:14 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8260 steps:31 episode_reward:0.10000000149011612 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:8270 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.011900000218302011,individual episode rew.[0.   0.09]\n",
      "Episode:8280 steps:15 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8290 steps:13 episode_reward:0.0 last n mean rew.0.012800000235438347,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8300 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8310 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8320 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8330 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8340 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8350 steps:15 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8360 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8370 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8380 steps:39 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8390 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8400 steps:15 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8410 steps:15 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8420 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8430 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8440 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8450 steps:15 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8460 steps:15 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8470 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8480 steps:15 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8490 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:8500 steps:14 episode_reward:0.0 last n mean rew.0.005600000098347664,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8510 steps:15 episode_reward:0.0 last n mean rew.0.005600000098347664,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8520 steps:14 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8530 steps:14 episode_reward:0.0 last n mean rew.0.0018000000342726708,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8540 steps:61 episode_reward:0.20000000298023224 last n mean rew.0.004700000081211329,individual episode rew.[0.09 0.2 ]\n",
      "Episode:8550 steps:14 episode_reward:0.0 last n mean rew.0.005600000098347664,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8560 steps:14 episode_reward:0.0 last n mean rew.0.007500000130385161,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8570 steps:14 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8580 steps:14 episode_reward:0.0 last n mean rew.0.006600000113248825,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8590 steps:16 episode_reward:0.0 last n mean rew.0.007600000128149986,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8600 steps:14 episode_reward:0.0 last n mean rew.0.010300000179558993,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8610 steps:14 episode_reward:0.0 last n mean rew.0.012100000213831664,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8620 steps:14 episode_reward:0.0 last n mean rew.0.01580000028014183,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8630 steps:14 episode_reward:0.0 last n mean rew.0.01580000028014183,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8640 steps:14 episode_reward:0.0 last n mean rew.0.012900000233203172,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8650 steps:14 episode_reward:0.0 last n mean rew.0.012000000216066837,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8660 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8670 steps:14 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8680 steps:14 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8690 steps:13 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8700 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8710 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8720 steps:15 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8730 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8740 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8750 steps:15 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8760 steps:15 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8770 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8780 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8790 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8800 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8810 steps:14 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8820 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8830 steps:14 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8840 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8850 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.007300000134855509,individual episode rew.[0.   0.09]\n",
      "Episode:8860 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8870 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8880 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8890 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8900 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8910 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8920 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8930 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8940 steps:15 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8950 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:8960 steps:13 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[ 0.   -0.01]\n",
      "Episode:8970 steps:34 episode_reward:0.09000000171363354 last n mean rew.0.008200000151991843,individual episode rew.[0.   0.09]\n",
      "Episode:8980 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.009100000169128179,individual episode rew.[0.   0.09]\n",
      "Episode:8990 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9000 steps:14 episode_reward:0.0 last n mean rew.0.012800000235438347,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9010 steps:14 episode_reward:0.0 last n mean rew.0.012800000235438347,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9020 steps:14 episode_reward:0.0 last n mean rew.0.012800000235438347,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9030 steps:14 episode_reward:0.0 last n mean rew.0.013800000250339508,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9040 steps:15 episode_reward:0.0 last n mean rew.0.012900000233203172,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9050 steps:15 episode_reward:0.0 last n mean rew.0.012900000233203172,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9060 steps:15 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9070 steps:14 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9080 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9090 steps:15 episode_reward:0.0 last n mean rew.0.005500000100582838,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9100 steps:15 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9110 steps:14 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9120 steps:14 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9130 steps:14 episode_reward:0.0 last n mean rew.0.002700000051409006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9140 steps:14 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9150 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9160 steps:13 episode_reward:0.0 last n mean rew.0.002700000051409006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9170 steps:15 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9180 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9190 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9200 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:9210 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9220 steps:14 episode_reward:0.0 last n mean rew.0.006300000119954348,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9230 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9240 steps:15 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9250 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.009200000166893006,individual episode rew.[0.   0.09]\n",
      "Episode:9260 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9270 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9280 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9290 steps:14 episode_reward:0.0 last n mean rew.0.00830000014975667,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9300 steps:29 episode_reward:0.10000000149011612 last n mean rew.0.011200000196695328,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:9310 steps:15 episode_reward:0.0 last n mean rew.0.013900000248104333,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9320 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.015700000282377006,individual episode rew.[0.   0.09]\n",
      "Episode:9330 steps:14 episode_reward:0.0 last n mean rew.0.014700000267475843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9340 steps:14 episode_reward:0.0 last n mean rew.0.014700000267475843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9350 steps:14 episode_reward:0.0 last n mean rew.0.013800000250339508,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9360 steps:14 episode_reward:0.0 last n mean rew.0.013800000250339508,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9370 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.014700000267475843,individual episode rew.[0.   0.09]\n",
      "Episode:9380 steps:30 episode_reward:0.09000000171363354 last n mean rew.0.015600000284612179,individual episode rew.[0.   0.09]\n",
      "Episode:9390 steps:14 episode_reward:0.0 last n mean rew.0.013800000250339508,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9400 steps:14 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9410 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9420 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.007300000134855509,individual episode rew.[0.   0.09]\n",
      "Episode:9430 steps:14 episode_reward:0.0 last n mean rew.0.007300000134855509,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9440 steps:14 episode_reward:0.0 last n mean rew.0.007200000137090683,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9450 steps:29 episode_reward:0.09000000171363354 last n mean rew.0.008100000154227018,individual episode rew.[0.   0.09]\n",
      "Episode:9460 steps:14 episode_reward:0.0 last n mean rew.0.008100000154227018,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9470 steps:14 episode_reward:0.0 last n mean rew.0.009900000188499689,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9480 steps:14 episode_reward:0.0 last n mean rew.0.010000000186264514,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9490 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9500 steps:15 episode_reward:0.0 last n mean rew.0.011900000218302011,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9510 steps:15 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9520 steps:15 episode_reward:0.0 last n mean rew.0.01010000018402934,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9530 steps:15 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9540 steps:14 episode_reward:0.0 last n mean rew.0.009200000166893006,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9550 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9560 steps:14 episode_reward:0.0 last n mean rew.0.008400000147521496,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9570 steps:14 episode_reward:0.0 last n mean rew.0.00570000009611249,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9580 steps:14 episode_reward:0.0 last n mean rew.0.004700000081211329,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9590 steps:14 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9600 steps:15 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9610 steps:14 episode_reward:0.0 last n mean rew.0.003700000066310167,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9620 steps:15 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9630 steps:14 episode_reward:0.0 last n mean rew.0.004600000083446503,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9640 steps:32 episode_reward:0.09000000171363354 last n mean rew.0.004600000083446503,individual episode rew.[0.   0.09]\n",
      "Episode:9650 steps:14 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9660 steps:14 episode_reward:0.0 last n mean rew.0.0036000000685453416,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9670 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9680 steps:15 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9690 steps:14 episode_reward:0.0 last n mean rew.0.005400000102818012,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9700 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9710 steps:15 episode_reward:0.0 last n mean rew.0.004500000085681677,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9720 steps:14 episode_reward:0.0 last n mean rew.0.005600000098347664,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9730 steps:14 episode_reward:0.0 last n mean rew.0.006500000115484,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9740 steps:16 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9750 steps:14 episode_reward:0.0 last n mean rew.0.007400000132620334,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9760 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.01010000018402934,individual episode rew.[0.   0.09]\n",
      "Episode:9770 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9780 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.012800000235438347,individual episode rew.[0.   0.09]\n",
      "Episode:9790 steps:14 episode_reward:0.0 last n mean rew.0.012800000235438347,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9800 steps:14 episode_reward:0.0 last n mean rew.0.01660000029951334,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9810 steps:14 episode_reward:0.0 last n mean rew.0.018600000329315663,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9820 steps:15 episode_reward:0.0 last n mean rew.0.017500000316649677,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9830 steps:14 episode_reward:0.0 last n mean rew.0.018500000331550838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9840 steps:14 episode_reward:0.0 last n mean rew.0.019400000348687173,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9850 steps:15 episode_reward:0.0 last n mean rew.0.018500000331550838,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9860 steps:14 episode_reward:0.0 last n mean rew.0.016800000295042992,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9870 steps:31 episode_reward:0.09000000171363354 last n mean rew.0.016800000295042992,individual episode rew.[0.   0.09]\n",
      "Episode:9880 steps:14 episode_reward:0.0 last n mean rew.0.014100000243633986,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9890 steps:14 episode_reward:0.0 last n mean rew.0.014100000243633986,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9900 steps:14 episode_reward:0.0 last n mean rew.0.013000000230968,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9910 steps:15 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[ 0.   -0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:9920 steps:14 episode_reward:0.0 last n mean rew.0.011000000201165675,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9930 steps:14 episode_reward:0.0 last n mean rew.0.01090000020340085,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9940 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9950 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9960 steps:31 episode_reward:0.10000000149011612 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.1 ]\n",
      "Episode:9970 steps:14 episode_reward:0.0 last n mean rew.0.008200000151991843,individual episode rew.[ 0.   -0.01]\n",
      "Episode:9980 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:9990 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[-0.01  0.  ]\n",
      "Episode:10000 steps:14 episode_reward:0.0 last n mean rew.0.009100000169128179,individual episode rew.[ 0.   -0.01]\n"
     ]
    }
   ],
   "source": [
    "def main(agent1,agent2,episodes,visualize_every,print_every):\n",
    "    total_rewards = [[],[]]\n",
    "    critic_losses = []\n",
    "    actor_losses = []\n",
    "    episode_single_scores = [] # the max between scores of both agents\n",
    "    episode_window_scores = deque(maxlen = DESIRED_EPISODES_AVERAGE)\n",
    "    \n",
    "    for episode in range(1,episodes +1):\n",
    "        finished = False\n",
    "        step_count = 0\n",
    "        episode_scores = np.zeros(num_agents)\n",
    "        \n",
    "        agent1.noise.reset()\n",
    "        agent2.noise.reset()\n",
    "        train_mode = not(episode%visualize_every == 0)\n",
    "        env_info = env.reset(train_mode = train_mode)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        while not finished:\n",
    "            actions_ag1 = agent1.act(states[0])\n",
    "            #print(actions_ag1)\n",
    "            #actions_ag2 = agent2.act(states[1])\n",
    "            actions_ag2 = np.random.randn(1, action_size)\n",
    "            env_info = env.step(np.concatenate((actions_ag1,actions_ag2),axis=0))[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            #add experience to shared replay buffer\n",
    "            #if np.sum(rewards) != 0.0:\n",
    "            replay_buffer.add(states[0],actions_ag1,rewards[0],next_states[0],dones[0])\n",
    "            replay_buffer.add(states[1],actions_ag2,rewards[1],next_states[1],dones[1])\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            episode_scores += rewards\n",
    "            finished = dones[0] or dones[1]\n",
    "            \n",
    "            if step_count % TRAIN_EVERY == 0 and len(replay_buffer) >= BATCH_SIZE:\n",
    "                train(critic_losses = critic_losses,actor_losses = actor_losses)\n",
    "                \n",
    "            if step_count % UPDATE_TARGET_NETWORK_EVERY == 0 and len(replay_buffer) >= BATCH_SIZE:\n",
    "                update_target_network(actor_local,actor_target,TAU)\n",
    "                update_target_network(critic_local,critic_target,TAU)\n",
    "                \n",
    "            step_count += 1\n",
    "            \n",
    "            \n",
    "        episode_single_score = max(episode_scores)\n",
    "        episode_window_scores.append(episode_single_score)\n",
    "        episode_single_scores.append(episode_single_score)\n",
    "        \n",
    "        total_rewards[0].append(episode_scores[0])\n",
    "        total_rewards[1].append(episode_scores[1])\n",
    "        \n",
    "        if episode % print_every == 0:\n",
    "            print(\"Episode:{} steps:{} episode_reward:{} last n mean rew.{},individual episode rew.{}\".format(episode, step_count,\n",
    "                                                                                    episode_single_score,np.mean(episode_window_scores),episode_scores))\n",
    "                \n",
    "        \n",
    "        if np.mean(episode_window_scores) >= DESIRED_AVERAGE_SCORE:\n",
    "            print(\"Solved in {} episodes\".format(episode))\n",
    "            checkpoint_name = \"checkpoint_solved_\"+str(episode)+\".pth\"\n",
    "            break\n",
    "            \n",
    "    return total_rewards,critic_losses,actor_losses,episode_single_scores\n",
    "\n",
    "train_rewards,critic_losses,actor_losses,episode_single_scores = main(agent1,agent2,10000,VISUALIZE_EVERY,PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n",
      "Score (max over agents) from episode 6: 0.0\n",
      "Score (max over agents) from episode 7: 0.0\n",
      "Score (max over agents) from episode 8: 0.0\n",
      "Score (max over agents) from episode 9: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    agent1.noise.reset()\n",
    "    agent2.noise.reset()\n",
    "    while True:\n",
    "        \n",
    "        actions_ag1 = agent1.act(states[0])\n",
    "        #actions_ag1 = [[1,-1]]\n",
    "        actions_ag2 = agent2.act(states[1])\n",
    "        #actions_ag2 = np.random.randn(1, action_size)\n",
    "        #actions_ag2 = [[1,-1]]\n",
    "        #print(actions_ag1,actions_ag2)\n",
    "        env_info = env.step(np.concatenate((actions_ag1,actions_ag2),axis=0))[brain_name]\n",
    "        next_states = env_info.vector_observations        # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        #print(states[0],states[1])\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent 1 rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_rewards[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent 2 rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_rewards[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### episode rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_single_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5wcdZnv8c9jAgEFAkIOy9WgRN3gCmrEy1GPCrvcdjceF9agx2WPuJxFcF3cdQ3KspA1ykVB0QQEEkBAkhAuBggJlyQkISSTyf06yWRmMpnck7nlNpO5/M4fXRN6Ol3d1d1VXd0z3/frlVdqqqt+9XRPTz1V9buZcw4REZF03hN3ACIiUrqUJERExJeShIiI+FKSEBERX0oSIiLia2DcAYThtNNOc0OHDo07DBGRsrJkyZI9zrkhmbbpE0li6NChVFZWxh2GiEhZMbPN2bbR4yYREfGlJCEiIr6UJERExJeShIiI+FKSEBERX0oSIiLiS0lCRER8KUmE6HBnN1Mqt6Dh10Wkr+gTnelKxbjZ1fzmzY0MGvgeRl50VtzhiIgUTHcSIdqzvx2A1rbOI+u6uh31ew/GFZKISEGUJCJ278wqvnTvbLY0KlGISPlRkojYOzV7gXfvMkREyomSRJE4oL2zK+4wRERyoiRRJL+cWcVHbpvBocNKFCJSPpQkimTBpsRjp/3tnVm2FBEpHUoSIiLiK1CSMLPLzazKzKrNbHSa1weZ2WTv9UVmNjTptVu99VVmdlm2Ms3sEjNbambLzWy+mZ1f2FsUEZF8ZU0SZjYAGAdcAQwHrjWz4SmbXQ80OefOB+4H7vb2HQ6MAi4ALgfGm9mALGU+CHzLOXcR8EfgtsLeooiI5CvIncTFQLVzrsY5dxiYBIxM2WYk8IS3PBW4xMzMWz/JOdfunKsFqr3yMpXpgJO85cHAtvzemoiIFCrIsBxnAVuSfm4APuO3jXOu08xagFO99QtT9u0Zr8KvzO8C083sENAKfDZdUGZ2A3ADwLnnnhvgbYiISK5KseL6FuBK59zZwGPAfek2cs497Jwb4ZwbMWTIkKIGKCLSXwRJEluBc5J+Pttbl3YbMxtI4jHR3gz7pl1vZkOAC51zi7z1k4HPB3onIiISuiBJYjEwzMzOM7NjSVRET0vZZhpwnbd8NTDLJcbLngaM8lo/nQcMAyoylNkEDDazD3tl/SWwLv+3V3ocGkZcRMpH1joJr47hZmAmMACY6JxbY2ZjgErn3DRgAvCkmVUDjSRO+njbTQHWAp3ATc65LoB0ZXrr/wl4zsy6SSSN74T6jkVEJLBA80k456YD01PW3Z603AZc47PvWGBskDK99S8ALwSJS0REolWKFdciIlIilCRERMSXkoSIiPhSkhAREV9KEiIi4ktJQkREfClJFJv60olIGVGSEBERX0oScsRf3vcWXx//dtxhiEgJCdTjWvqHjbv2xx2CiJQY3UmIiIgvJYkoONVOi0jfoCQRIrO4IxARCZeShIiI+FKSEBERX0oSRabaChEpJ0oSUVMltoiUMSUJERHxpSQhIiK+lCRERMSXkkSIVP0gIn2NkkQUMvSqU387ESknShIiIuJLSUJERHwpSYiIiC8liSJT3baIlBMlCRER8aUkISIivpQkCvTHRfUsrmuMOwwRkUgoSeSo+eBhXlm5/cjPP3lhFdc89E6MEYmIRGdg3AGUm4vGvJ74/9yvctbJx8ccjYhItHQnkafDnd1xhyAiEjklCRER8aUkUWQ3PrWEC+98Le4wREQCUZ1EDr7/zLJA23V0dtPV7RjwnqOH81ta3xx2WCIikdGdRAYdXd10dr1b9/DSim2B9hvz8lq+9ejCqMISESkaJYkMhv30Vf7y/rl57buwRn0nRKT8KUlkUbvnQNwhiIjEJlCSMLPLzazKzKrNbHSa1weZ2WTv9UVmNjTptVu99VVmdlm2Mi1hrJltMLN1ZvYvhb1FERHJV9YkYWYDgHHAFcBw4FozG56y2fVAk3PufOB+4G5v3+HAKOAC4HJgvJkNyFLmPwLnAB91zv05MKmgd5iDA+2dPDhnE13dpTFWa1tHFw/O2dSrXkREpJiC3ElcDFQ752qcc4dJnLRHpmwzEnjCW54KXGJm5q2f5Jxrd87VAtVeeZnKvBEY45zrBnDO7cr/7eXmnhnruXvGeqav2p594zRSZy3d19bBioaWvON5cM4m7p6xnmcWb8m7DICH525i+Ra1qhKR3AVJEmcByWepBm9d2m2cc51AC3Bqhn0zlfkh4BtmVmlmr5rZsHRBmdkN3jaVu3fvDvA2stvX3glAe0i9qe97fUNB+x/w4mk73FVQOT+fvp6vjXu7oDJEpH8qxYrrQUCbc24E8AgwMd1GzrmHnXMjnHMjhgwZUtQAg9LQHSJS7oIkia0k6gh6nO2tS7uNmQ0EBgN7M+ybqcwG4Hlv+QXg4wFiLAlvrivakzERkaIIkiQWA8PM7DwzO5ZERfS0lG2mAdd5y1cDs5xzzls/ymv9dB4wDKjIUuaLwFe85f8FFPbMJg8T5tfy78+uyHm/7S1tEUQjIhKfrMNyOOc6zexmYCYwAJjonFtjZmOASufcNGAC8KSZVQONJE76eNtNAdYCncBNzrkugHRleoe8C3jazG4B9gPfDe/tBrNueyvrtrcWXM7Ti+pDiEZEJD6Bxm5yzk0Hpqesuz1puQ24xmffscDYIGV665uBq4LEFTbj6LGWSoGjNJrkikj/U4oV12VjRYjNShds2sM1Dy3o1Seip0ntz6evZ1l9U2jHEhEJSkkigPbO9E1QR4bYrPSHk1ewuK6J3fvb077+1oZwmvmKiOSi3yeJqh372NJ4MOM2t72wukjRZLaqoYX6vZljFREJU7+fT+KyXydGea27y78apFR6K//N7+YDmWMVEQlTv7+TSJY6rIaISH+nJBGzDTv3kehSktmv39hYhGhERHpTkojR/I17+Kv75zJ58Za0zVytgFubto4u1m4rvK+HiPRvShIxqtmzH4A1SSfzsPpq/NuzK7jygXk0HTgcSnlx29HSxpPv1MUdhki/0+8rroOIuivbkws3h17m0s2JfhWHOgobQbZUXP/EYtZsa+XS4adzxuDj4w5HpN9QkkiS6Rq+rkjTmH72F28CMO6bnyzK8cpF88EOgJKZEEqkv9DjpgAM+PIv5xT1mFOXFDbRkIhIGJQkRETEl5KElJUArYVFJERKEiVMfftEJG5KEgFEdfFaClfF3d2OlkMdcYchIiVKSSJGj71dG3cI3DOzigvvfE2JQkTSUpKIUV0RRnTNdvJ/eeU2AFqVJEQkDSWJANLVDZRLfcEVv5kXaLvZVbsijkREypGSRJmr23OAbc2H8tq3q9vR0JTY9/Y/rcmydbw0Qq9IPNTjOonfiSi2+uUAJ8aeTn75zDHxTEV9zvuISP+iO4kSVcgIsEE1H+wbg/+JSHSUJERExJeShEiMnHPMWr8z0MRTInFQkihROmn0D5MWb+E7j1fy7JKGuEMRSUtJIklYE/4UQ74tmqS09Pwed7S0xRyJSHpKEiXML2n9aflWPn/XLBbW7C1yRCLS3yhJlKGeWefWb/efwzrIPVExWlAVW1e3o72zb8zGJ1IKlCQkkO5ux32vVbGrtbDHIotq9tLalv8QINmqaq6bWMFHbpuRd/ki0puShASybEsTD8yq5oYnl+Rdxv72Tr7x8EJu+ENlzvsGvemZX70n57Kl/9izv72ox1tYs7fs72yVJDwthzpKauiHII+CltQ3FyGShK7uxP/Lt2Q/5uqtLWxpPHrwwo7ORCHrd+wLNTaRIF5euY0RP3uDitrGohxv3fZWRj28kLGvrCvK8aKiJOGpL8KIrGF7acW2uENI669/O58v3jM77jBEelnsJYe121qKcrymA4kRDTbu3F+U40VFSSImv5xZVfRjjptdHWi7Um6O6eIbSUukX1KSSFLM/mu/C3DCDvvx170zq9gb4JlszxwTpaSc+rCI9CX9Okk8v/TdXq66QhUROVq/ThJPvLP5yPLf/u5tavaUx7PDvti/QURKU79OEqkW1zXFHYL40J2elJu+8o1VkpCS4Zyjq7uv/GmJJJT7jb+SRABxjMjaH0eBvfX5VXzoJ9PTvqaKa5F4BEoSZna5mVWZWbWZjU7z+iAzm+y9vsjMhia9dqu3vsrMLsuhzAfMrDwqCSLS306LkxZviTsEKSHdGoerJGRNEmY2ABgHXAEMB641s+Epm10PNDnnzgfuB+729h0OjAIuAC4HxpvZgGxlmtkI4JQC31tWKwL0HoZ4KorNLPJnmnHdBvfDmyTJw50vreEjt82gW48gYxXkTuJioNo5V+OcOwxMAkambDMSeMJbngpcYokz60hgknOu3TlXC1R75fmW6SWQe4H/KOytieTuhWUNvLF2Z9GO1xcSZvWufbR1hH/F/9SiegC6+8KHVMaCJImzgOTnAA3eurTbOOc6gRbg1Az7ZirzZmCac257pqDM7AYzqzSzyt27dwd4GxK25oOHGTr6FWav35XTfoXcwUR9vrhl8gq+m8cAhIUyEvVQE+fXBurwGKXOrm6mLmkIdAXfcqiDS++by39MXVmEyCQOJVVxbWZnAtcAv822rXPuYefcCOfciCFDhkQfXD8S9PHaWm8+i9/P3RRlOED5txAJYt32fYx5eS3/Onl5rHFMmF/Lvz+7gi/cPSvrtocOJ+4gFtVqAqxUfeUGKEiS2Aqck/Tz2d66tNuY2UBgMLA3w75+6z8BnA9Um1kd8F4zCzbgUISqd/Wd+vM+8r3tkzq8oXZbDuU/30Y61bv2sb3l6OlutzQeZFGa2Q33egPTbSvhMbzKSblf4ARJEouBYWZ2npkdS6IielrKNtOA67zlq4FZLtGGcxowymv9dB4wDKjwK9M594pz7s+cc0Odc0OBg15leL9UyHcr7Mr2cv+il5Owr0AvvW8un/vF0XcFX7xnNt94eGG4B8tRZV2j72jGUTUDL9aFUl/pAJo1SXh1DDcDM4F1wBTn3BozG2Nmf+ttNgE41bvq/yEw2tt3DTAFWAvMAG5yznX5lRnuW5Ns8u17ULfnQK+Kyr5yWx2V9s4uHplbQ2fPpBw+0iXiA+2daefm6Cuufugdvv/MsozbhHHB09B0sNcwPMVU7n18BgbZyDk3HZiesu72pOU2EnUJ6fYdC4wNUmaabU4IEp+EK9tX+su/nMMVH/sz/uFzQ4sRTi+55KOl9U04B5/6QOStqdOq3rWfIScO4okFddz3+gaOO3YA3/7sB7Lul3wF+s1HF7FiSzN1d10VZah9XlXSRFfdDr76yzn84NJhjLwotQ2OpCqpimt516wcWwyFaczLa3v9UR083MmC6t7PrudtLO40oflci319/AL+7sEFoccS1KX3vcXVDy5gf3snAAe9/9OpqGtMe8UZtC9P3MrpbrKzq5uaPQf40bNqkRWEkkRISnWWuHxd9uu5R5Z/9OxK7n9jQ4zRlK+NARs9zNu4h8V1iZnTyumEG2VdVRl9DGmV0+8xEyWJPKX+cWR7rhrlsfP1p+WpjdTS27Az85zUxfhb6CN/bxnt2hdv/4hS5fd1r6xrLOlZFHuUe6MPJYl+bP2OzCf/bJK//Bt27jvyWEUKU+4nlWK5+qF3uORXcwJt21eu6uOgJJGn19ZEP3SD38li896jW7vsaGlL2+Y91eTFW5hdtYufT19XaHi9/NX9c/mHCYtCLTNZXzhvZjtPPfRWolNiOZ3Q4o71wOH8hwMplSaq/zZlBS8uC3ZXH4dArZvkaGNDPsnmIl2l9md/8SYAdXddxdDRr/jue+/MqsjiWlrfzGNv1wbe3jnHnv2HGXLioLyO13jgMD99YRV3X/1xTjrumLzKKIa+kOCyCePuxznHhPm1fO0TZ3HaCfl9J3zLTloutTu155Y28NzSBr72idJsaaU7CSlYRW3jkeU7X1obeL8J82v59Ng3qNmdX4/28bOreXX1DiZX9K0hxuO+Oo/rHLp2eys/e2Ud/zopMSxJ3J9Dqoraxl6t/voLJQnJi3Ou4JPJWxsSAzNuaTp6yAgpDdW79lG750DW7cI4oXd2JQppbQt3WJKw/P3v3+nV6q+/UJKQrA6UUIV0nDP2bWs+xANvbsw7hqC7ldIF9KX3zeUrv5zj+7rfo5vnlzZw24urogkqD7HMLln0I0ZDSUKyKoWB3uKY+CnVjU8v5b7XNwTu+3BE/KFHbte+dj5+x0zqvUYVP5yygqcW1udVVrHO52Ed59F5Ndz09NJwCitBShIlLJ8xX8Kc7vFXr0VXyZ2vng5nuThUQAuYZG1eOZoEJ2HJ5kaaD777aKi1rZPnlzXkXV6xrgPCHkvpZ6+s45VVGae/Cc2B9s7A/ZvCoiRRwvJpovele2aHdvzfzso8Snu2TnZB5fIn++Pncn+EcfBw6TwuS1UqzTDz8XcPvsO3Ho13FNlyMn5ONc9U5Hd31eO2F1fzg0nLWV7E4VqUJErYuNm5T+azszXcXrsT56dv0nq4q5v//FNuA/d+4r9fD7Rdpia8hTjc2R1rnUZftGf/4dDLjCJxul7L/uUvrW/i2cpEa7n6vQe5Y9qao2boW7OthboAlfmpdXn3zKji1ucLq6fpmRekmBc+ShKS0USffg8dXYX/IRfrfN3Z7Tjc2c2Hb3s19E6EuUh3ckr36CNdIpvgk6z9tHV0sWtf/HVJuYhySO2g37Wvj1/Aj7ypWG9+ZimPL6hj9baWXttc9cB8vpyhMr/H97x6ilKoTyuEkoTELtvfUFtH15FZ29LJduX58+nrjtTVPDKvlhmrd+QcYzo1u/cHGjsojJPff78cvP8JwLceXcTFY98s+LilKow5NrLljZ66p3KfD6JQShJS8j76nzNoSNOXIugF2rL63s9v//mpJWGExVd/9daRnu5ha+voOtKPJB9LNjeFGE1w+Z5QWw4G7xsxY/V2vhiw7u3VVdtZVt/7syj2Sb/cU4yShEiOivGY7M6X1nDdxIroD5ROAWe1fOsTLhzzGks2vztU+tQl/q2kVm1t8X0t1Y1PL+V/jy9sTpG4Ghes2dbCuu2tsRw7mZKEZJTuCj4s5dayp1iPlp2DTbuzV4z2Nau3vXtCfHRezZHlMD735LvHoN+7fO44XljWwJptwZNYJlc9MJ8rfjMvlLIKoSQhkTrQ3klXd+Y/yqhv/0ul3jDuhlWfHvtGn+70lasovha3TF7BVQ/Mj6Dk+ChJSKQu+K+Z/OjZFbHGEPfJOawk1drWwV//dh7Vufb49uze1x55p68wEn7RelxneO2Pi+qz3nG0tnXk1Vy7u9vlVAcTNyUJidzzeYyVr/4MR5u9fhert7bymzc3xh1KYPfMWF+Wv8ufvLCK7c2Jlmt+PfYbGvN7FPvArI1cOOY1dpfJTIRKElJ0zQc7mLJ4S8YrxsV14bXOiftx0yNza7Jv1EeNn7OJDTvzu/OJW5f3Bf3GwwuP6lCXTabObj1NsPNJEnHkWyUJicV/PLfyyPLqbS08OKd37/JDHeGNQWXE26GpM8cTTFwV+tW7opmCttwaKKST6zsYfvvMI8txX6QUSjPTSVGkuxJbsCkx3epdr64/6rWWQx1Mqqhn1MXnZi0729VV3d6D/OaNDcECDWBRzV4++mcnhlZeOunOKwuqs09PW4hL75vLp4eewic/cEqkx8mmWCklW4OKMLUcOroOIp/kGUfCUZKQorhrxtGJIJNbJi+nq9vx52ec5LtNLncHj8zLbViLHmu3tXLeae/j+GMHHFl3x0trOSHC6VL9kt7kyuhn4Ftc1xRbkojjgvulFdv4mwvPzLpdofUqf1z07sB+5TZMhx43SVE8nONz+Z6rvEczjFmUPDTD9pZDzFgdbsudfW0dXPnAPH4wadlRr9XnMSxEe6f/0CJ9WdAWT73noS78RLpnf/Zn/gtr/O/OyutUHh0lCSlp+zNMZflq0hhMVz/4Dv/81Lt9AH48dWW6XXLS1pE4qS+tD6cS/YEyaZXUF8YqqqhtZMTP3iioDOezHJZ8bk5UcS0SwOGUK/Ka3QfY2ty7OWIxHs1EZVeWVi+lfAqP+knK1uZDgYbQz9TrOe4WuWF8RMVM5EoSUnZSR16NOiFEeVJJV3mZrpJTEqp2FG8so7BOw1GczovZYkxJQiIT1eRBxVJm9YtpdWYYYr1Ycv0cw+h8F/avrpCQUutXCvlexfGdVJKQkpZrH4OgVnjTP85YvZ2/uv+tnDtLRf23GtbJIJ9WXZv35j+4YNyPcoop399Rrp+Rc45Z63fm/B0Ni5KElLR5G/cctS6MW+2R494G4IdTVrBh5/5AnfeSrwgLbam0q7WNhTV72dac++xxucwz0XQw9+lFXw1pUqZ8dTvnewcU9rP4cshpL6/czncer+SxBXWxJGH1k5Cy8+CcTdz1dx8v2vEcsKB6T6+x/Q8VOMfwxT/Pf7KilkMdVNQ2cvF57y8ohiiku7rOtRd3GMN4hNGENrmMMOsAsoW2rfkQ7xv07ql5Z2viQmJr0rD9qrgWyWDdjn2hlZXpyiz5z/Cbjy4K5XgX3D4jlHIaDwS7Q4hjELntLb1bmn19/ALerj76jjAvAc+NmU7EQfNHXAMTfv6uWXwlzRzacQ1voiQhQmF1AE8t3Bx42wM+I4pGpTuGE931j1cetW5xXWPW/fLpoOgn06+zVOpN0sXR09gj+SIg3V2RWjeJFNnw22f26sGdLNsV5W0vro4ipCzCO0nU7w3v5PzUws20pukA+daG3Ywc9zYdGVpbVdRmTyRhm7VuF0NHv5I25mQFtW5K/bmACm+1bhIpsuQrstlVu3q9lum59tuboh1sr5jPnL9072zfBJkrv46Ay+qbWbGlmScW1BVUfuBPJeDZdIf3vH9TmomcSmWMpeQoSrbHtZldbmZVZlZtZqPTvD7IzCZ7ry8ys6FJr93qra8ys8uylWlmT3vrV5vZRDOLbiQ16ffy/aPLd3a4Ygt6mgsyzlFQmeZFj2Io8h5VO/Yxb2Pwll/ZRJ0iCnlk1NXtGDr6FX7zRvRDvWRNEmY2ABgHXAEMB641s+Epm10PNDnnzgfuB+729h0OjAIuAC4HxpvZgCxlPg18FPgL4HjguwW9Q+lzovrj9UsYTXlMNVloR8LdIZ20X1y+LZRySkWmq/vLfj2Xb0+oSGxXrICSLNgUrHI+jLvEntnyHp0X/YRWQe4kLgaqnXM1zrnDwCRgZMo2I4EnvOWpwCWW+G2OBCY559qdc7VAtVeeb5nOuenOA1QAZxf2FqWvCfOOO9Mz8jgV+07lpRXRzn1diHzuPnJ9UpTPo6XUXb75SLAWcLneQaQLrZhPnYIkibOA5MFxGrx1abdxznUCLcCpGfbNWqb3mOnbQNo2g2Z2g5lVmlnl7t3h3WJK6QvzKjFTJ9bSeCJdHC8uz30e8nzk83jvn59aEn4gOSqFFlHOOVVcpxgPzHXOzUv3onPuYefcCOfciCFDhhQ5NOkLgjTLLFWFnLTSVR4H7RNw41NLeKaiPvuGIXo7aUa+oOfITI900r3TdFuHNYXum+t30ZzU8z3Xx03lUHG9FTgn6eezvXVptzGzgcBgYG+GfTOWaWb/BQwBfhjkTYjk45qH3un1c6GdpypqG1nujQlVysbPqc5731dX7+DW51eFGE00wrjiPhhin5bfp5l0K+jXzaX8D+8mjmLkjCBJYjEwzMzOM7NjSVRET0vZZhpwnbd8NTDLq1OYBozyWj+dBwwjUc/gW6aZfRe4DLjWOVeaD4wlVlHdcqf+weV6nL///Tt8zRsTKpPH3s5vKlU/c6oSbf1zGZjvh1OW9/o5yMkmjFF9i3UhnOtXpJiPcXI91n+/vDZNGcULOOvYTc65TjO7GZgJDAAmOufWmNkYoNI5Nw2YADxpZtVAI4mTPt52U4C1QCdwk3OuCyBdmd4hHwI2A+94H8Tzzrkxob1jER/JYzMBXDTm9cD75tLP4M6Xjv6jL8QLyxI34cvqm/nAqe/Lur1hPL+098OAXEcYTZ6zuZhKpOtCUcU0+OsRgQb4c85NB6anrLs9abkNuMZn37HA2CBleus16KDEInlY8tTZ77L54j2zww6npDSljBX1kxdK95HTogzzVucrimEwci0x+fFUMceVKuWKa5GiSr66nrqkIcZI8lPIiSzbnoUOjR6WIJW+//SHyqLccfxi+vqC9i9kQqiex03FSBZKElJ2ivHE4XBncQfiy1W6Sswec7PMN5HuBLqvLbqe0L3keVL78dSVHPQZnv1Py7fmVF8SVh+UXOb1SLayITH/9rjZuTUgSL4ImLV+Z17Hzoce7UjZWVrfzJLN5dt8NSo9599/mFgRbyAB7N7XzvNLg9+tTa7cwrDTT+C9xx59yvrDO71H4XVkvuNId5eY7Q7lrarw+2L5jXMVxDMV0c7rnkxJQsrS/3sy2g5WdSGOjBqF5Avy5JYuQepSSqHu91+eWcY7OdYd1DcePCoh+MrxTf7N7+bzo8s+4vv6jU8vza3APkSPm6Qs7dmf+7ScuXi8wNFK43KoyPNV5Gtfe+7jYW1vST/Va2qlOuSXCO+dWZXHXvkrl9+VkoRIyB6csynyY+xsbWNlQzNDR7/C6q2JZ9xR1mGGVRHcE2KYQ6HX7AneP6SU5Bp3ut9vqXSmE5Ec3D2jsFYvQYx5eS0vr0wMyrfRq4htOhjt3VWY8kk6gZNgCYyzlI9cK7KLRXUSIn3EgfZgjy9KZTKdKJXje8z2uOvpmDow6k5CpEylngaLOe9xoaI8hbuIyw/T/I3B5qCIk+4kRPqQ7a3+s8IVouVQ7hXN6by0Yht/fsZJeVZyBE+C5XIj8X8mBJuDwk8xOl7rTkKkXFnqj8arq3Zk3S2fXrqjn1uZ8z7p1O09yPfybE4aNOwyyQ9lQ0lCpEylthC6/40Nga6gt/k0Jc2kNeQe2SsiHFK9K6bJefoqJQkRyaqYA8oVKsx5IERJQqRspbtaDrP/QanZsGtf4G378ueQrBiNFZQkRPqQ+9/YEEm5pdCkdEtj8Er5Egi3z1CSEClTa7a1Zt+on/rBpOXZN5JAlCREylS2IcHDFNbw2hKuYjxWU5IQERFfShIiImVtqPAAAAl0SURBVGVKFdciIuKruwizyipJiIiUqcMFzJMdlJKEiIj4UpIQERFfShIiIuJLSUJERHwpSYiIiC8lCRER8aUkISIivpQkRETEl5KEiIj4UpIQERFfShIiIuJLSUJERHwpSYiIlLHqHOb+zoeShIhIGausa4q0fCUJEZEydsyAaE/jShIiImUs6jkllCRERMrY3v3tkZYfKEmY2eVmVmVm1WY2Os3rg8xssvf6IjMbmvTard76KjO7LFuZZnaeV0a1V+axhb1FEZG+a2Dcj5vMbAAwDrgCGA5ca2bDUza7Hmhyzp0P3A/c7e07HBgFXABcDow3swFZyrwbuN8rq8krO1K1v7iSl7//hagPIyISuoHvsUjLD5KCLgaqnXM1zrnDwCRgZMo2I4EnvOWpwCVmZt76Sc65dudcLVDtlZe2TG+fr3pl4JX5tfzfXmav3fIlfv/tT2FmfOyswdTddVVUhxIRicQFZw6OtPyBAbY5C9iS9HMD8Bm/bZxznWbWApzqrV+Ysu9Z3nK6Mk8Fmp1znWm278XMbgBuADj33HMDvI2jffj0E/nw6Sf2WpecKDbt3s+Zg49n2ZYmhp76Ps48+XgAOrq6uWPaGj7zwVP5xDkn09rWQUVtI//4+aGYGd3djh2tbdTuOcDpJx1HR1c3O1rb+MpH/gdL65v42JmDOWaA0d7ZzbHerWK3cxxo72JHaxtDT3svzQc7cA7eY3DqCYN4j8G2ljZWbGlmW/Mhrv7U2QAMGjiAvQfaGXz8MdTsPkCXczQ0HeJDQ97Hue9/L3v2H2bp5iZa2zr4n+efxsKavZwx+HgMOOG4gTyxoI6PnTWYTbv2c+E5J3PVx89gVUMLG3buY/Dxx3D8sQOoqG1kyeYmzj7lvcyp2sVF55zM9y8ZxtwNu5kwv/bI53Xm4OPY1tLGqE+fwysrt3PBWSexsKaRyy44ne0tbXzk9BN5dkkD3/vyhwB47O06DnV0+f5+rr34XJ6pqOe4Y95DW0f3kfL7oqv+4gxeWbU97jCkDH3uQ6dGWr455zJvYHY1cLlz7rvez98GPuOcuzlpm9XeNg3ez5tInPTvABY6557y1k8AXvV2O6rMpO3P99afA7zqnPtYphhHjBjhKisrc3jbIiJiZkuccyMybRPkcdNW4Jykn8/21qXdxswGAoOBvRn29Vu/FzjZK8PvWCIiUiRBksRiYJjX6uhYEhXR01K2mQZc5y1fDcxyiVuUacAor/XTecAwoMKvTG+f2V4ZeGX+Kf+3JyIihchaJ+HVMdwMzAQGABOdc2vMbAxQ6ZybBkwAnjSzaqCRxEkfb7spwFqgE7jJOdcFkK5M75A/BiaZ2c+AZV7ZIiISg6x1EuVAdRIiIrkLq05CRET6KSUJERHxpSQhIiK+lCRERMRXn6i4NrPdwOY8dz8N2BNiOFFTvNEqp3jLKVZQvFHLJ94POOeGZNqgTySJQphZZbba/VKieKNVTvGWU6ygeKMWVbx63CQiIr6UJERExJeSBDwcdwA5UrzRKqd4yylWULxRiyTefl8nISIi/nQnISIivpQkRETEV79OEmZ2uZlVmVm1mY2OMY46M1tlZsvNrNJb934ze93MNnr/n+KtNzN7wIt5pZl9Mqmc67ztN5rZdX7HyyO+iWa2y5tcqmddaPGZ2ae891/t7VvQpL0+8d5hZlu9z3i5mV2Z9Nqt3rGrzOyypPVpvx/eEPeLvPWTveHuC4n3HDObbWZrzWyNmf3AW19yn3GGWEvy8zWz48yswsxWePHemekYlpjWYLK3fpGZDc33fYQc7+NmVpv0+V7krY/+u+Cc65f/SAxRvgn4IHAssAIYHlMsdcBpKevuAUZ7y6OBu73lK0nM7mfAZ4FF3vr3AzXe/6d4y6eEFN+XgE8Cq6OIj8QcI5/19nkVuCKCeO8A/j3NtsO93/0g4DzvOzEg0/cDmAKM8pYfAm4sMN4zgE96yycCG7y4Su4zzhBrSX6+3vs9wVs+BljkfQ5pjwF8D3jIWx4FTM73fYQc7+PA1Wm2j/y70J/vJC4Gqp1zNc65w8AkYGTMMSUbCTzhLT8BfC1p/R9cwkISM/mdAVwGvO6ca3TONQGvA5eHEYhzbi6JeUJCj8977STn3EKX+Ab/IamsMOP1MxKY5Jxrd87VAtUkvhtpvx/eVddXgalp3nu+8W53zi31lvcB60jM7V5yn3GGWP3E+vl6n9F+78djvH8uwzGSP/OpwCVeTDm9jwji9RP5d6E/J4mzgC1JPzeQ+cseJQe8ZmZLzOwGb93pzrnt3vIO4HRv2S/uYr+fsOI7y1tOXR+Fm71b8ok9j27yiPdUoNk51xlFvN7jjU+QuIIs6c84JVYo0c/XzAaY2XJgF4mT5aYMxzgSl/d6ixdT0f7uUuN1zvV8vmO9z/d+MxuUGm/AuHL+LvTnJFFKvuCc+yRwBXCTmX0p+UUv45dsW+VSj8/zIPAh4CJgO/CreMM5mpmdADwH/KtzrjX5tVL7jNPEWrKfr3Ouyzl3EXA2iSv/j8YcUkap8ZrZx4BbScT9aRKPkH5crHj6c5LYCpyT9PPZ3rqic85t9f7fBbxA4ou807s1xPt/l7e5X9zFfj9hxbfVW05dHyrn3E7vj68beITEZ5xPvHtJ3NIPTFlfEDM7hsRJ92nn3PPe6pL8jNPFWuqfrxdjMzAb+FyGYxyJy3t9sBdT0f/ukuK93HvM55xz7cBj5P/55v5dyFRh0Zf/kZjfu4ZEJVRPhdMFMcTxPuDEpOUFJOoS7qV3peU93vJV9K6oqnDvVlTVkqikOsVbfn+IcQ6ld0VwaPFxdEXalRHEe0bS8i0kni8DXEDvCskaEpWRvt8P4Fl6V3p+r8BYjcSz4V+nrC+5zzhDrCX5+QJDgJO95eOBecBf+x0DuIneFddT8n0fIcd7RtLn/2vgrmJ9F4p6Qiy1fyRaBmwg8YzypzHF8EHvi7UCWNMTB4nnoG8CG4E3kn7BBozzYl4FjEgq6zskKtSqgf8bYozPkHiE0EHiGeb1YcYHjABWe/v8Dm8kgJDjfdKLZyUwjd4ntZ96x64iqaWH3/fD+51VeO/jWWBQgfF+gcSjpJXAcu/flaX4GWeItSQ/X+DjwDIvrtXA7ZmOARzn/Vztvf7BfN9HyPHO8j7f1cBTvNsCKvLvgoblEBERX/25TkJERLJQkhAREV9KEiIi4ktJQkREfClJiIiILyUJERHxpSQhIiK+/j/RJCtvlkDdDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(critic_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(actor_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = OUNoise(size=action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(5):\n",
    "    print(\"--------------------------\")\n",
    "    noise.reset()\n",
    "    \n",
    "    for j in range(100):\n",
    "        sample = noise.sample()\n",
    "        print(\"iteracion {} sample {}\".format(i,sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704],\n",
       "        [0.89044853, 0.73811704]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1.actor_local.eval()\n",
    "agent1.act(replay_buffer.sample(100)[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
