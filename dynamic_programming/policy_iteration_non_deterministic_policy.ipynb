{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration in non-deterministic policy\n",
    "An implementation of policy iteration using a grid_world with non deterministic policy and \"iterative policy evaluation\",\"policy improvement\" already implemented in this repo(many code is repeated to maintain notebook independency and never needing to read 2 notebooks).\n",
    "\n",
    "In this case it is the grid world is a \"windy world\" that means, the agent decides to move up and with 0.5 chance it will go up, and 0.166666 will go in others directions due to wind(the agent moves in the selected direction 0.5 of the time only). Thats why this is a non deterministic policy and this probabilities are considered in \"policy evaluation\" and \"policy improvement\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gridworld has the shape(3,4) with a winning state \"w\"(0,3), and a lossing state \"l\"(1,3), a non valid state \"x\"(2,1) and a start state s(3,0)\n",
    "\n",
    "|  |  |  |  |\n",
    "|---|---|---|---|\n",
    "|  |  |  | w |\n",
    "|  |  |  | l |\n",
    "|  | x |  |  |\n",
    "| s |  |  |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import grid_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the maximum change before finish the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONVERGENCE_THRESHOLD = 10e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function to display the values of a policy after finishing iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_values(V,grid):\n",
    "    for i in range(grid.width):\n",
    "        print(\"--------------------------\")\n",
    "        for j in range(grid.height):\n",
    "            v = V.get((i,j),0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function to display a stochastic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(P,grid):\n",
    "    for i in range(grid.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(grid.height):\n",
    "            a = P.get((i,j),' ')\n",
    "            if isinstance(a,dict):\n",
    "                a = list(a)[0]\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From or defined grid world file, import a negative grid ,retrieve all actions and states and print grid rewards\n",
    "Negative grid is used to encourage the agent to find a shortest path to the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = grid_world.Grid.negative_grid(step_cost = -1.0)\n",
    "states = grid.all_states()\n",
    "actions = list(set([action   for action_tup in grid.actions.values() for action in action_tup]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'R', 'U', 'L']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards of negative grid\n",
      "--------------------------\n",
      "-1.00|-1.00|-1.00| 1.00|\n",
      "--------------------------\n",
      "-1.00| 0.00|-1.00|-1.00|\n",
      "--------------------------\n",
      "-1.00|-1.00|-1.00|-1.00|\n"
     ]
    }
   ],
   "source": [
    "print(\"Rewards of negative grid\")\n",
    "print_values(grid.rewards,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary function to compare if two policies are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equal_policies(policy1,policy2):\n",
    "    # policy1 and policy2 are dicts\n",
    "    \n",
    "    for s in grid.actions.keys():\n",
    "        for a in actions:\n",
    "\n",
    "            if policy1[s][a] != policy2[s][a]:\n",
    "                return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an initial random policy, and improve it in time using policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random policy is:\n",
      "---------------------------\n",
      "  D  |  U  |  L  |     |\n",
      "---------------------------\n",
      "  D  |     |  L  |     |\n",
      "---------------------------\n",
      "  L  |  D  |  D  |  U  |\n"
     ]
    }
   ],
   "source": [
    "policy = {}\n",
    "policy_probs = {}\n",
    "\n",
    "for s in grid.actions.keys():\n",
    "    action = np.random.choice(actions)\n",
    "    policy[s] = action\n",
    "    policy_probs[s] = {action:0.0 for action in actions}\n",
    "    policy_probs[s][action] = 1.0\n",
    "    \n",
    "print(\"Initial random policy is:\")\n",
    "print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a step_dynamics dictionary indexed with state,action\n",
    "\n",
    "For every state and selected action  it provides the probability of the real action taken due to wind.\n",
    "(0.5 probability of going in selected direction, 0.16666 in the other direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_step_dynamics(policy):\n",
    "    \"\"\"Policy is a double dictionary[s][a]\"\"\"\n",
    "    step_dynamics = {}\n",
    "    for state in policy.keys():\n",
    "        step_dynamics[state] = {}\n",
    "        for action in policy[state].keys():\n",
    "            step_dynamics[state][action] = {}\n",
    "            for real_action in policy[state].keys():\n",
    "                if  real_action == action:\n",
    "                    step_dynamics[state][action][real_action] =0.5\n",
    "                else:\n",
    "                    step_dynamics[state][action][real_action] =  0.5/3.0\n",
    "                \n",
    "           \n",
    "            \n",
    "    return step_dynamics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration is a combination of policy evaluation and policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the iterative policy evaluation algorithm:\n",
    "Function that receives as parameter a policy to evaluate, the disccount factor gamma, and the convergence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of initial random policy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.iterative_policy_evaluation.<locals>.<lambda>>,\n",
       "            {(0, 0): -9.070970487178233,\n",
       "             (0, 1): -8.155755330928118,\n",
       "             (0, 2): -6.011402074994884,\n",
       "             (0, 3): 0,\n",
       "             (1, 0): -9.169769891632736,\n",
       "             (1, 2): -5.152976403251383,\n",
       "             (1, 3): 0,\n",
       "             (2, 0): -9.018349775336329,\n",
       "             (2, 1): -8.21288709979621,\n",
       "             (2, 2): -6.216289489445616,\n",
       "             (2, 3): -2.7606304125528105})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iterative_policy_evaluation(policy,gamma = 1,convergence_threshold = 0.1):\n",
    "    V = defaultdict(lambda :0) # initializes to 0 the value of all states\n",
    "    \n",
    "    step_dynamics = create_step_dynamics(policy)\n",
    "    \n",
    "    while True:\n",
    "        max_change = 0 #the maximun differente between old and current value in an iteration\n",
    "        for s in grid.actions.keys(): #finds the value for every state\n",
    "            old_value  = V[s] #keep a copy of old value of state\n",
    "            \n",
    "            new_value = 0 #create an accumulator for the new value of the state after aplying bellman equation\n",
    "            for action in policy[s].keys(): #calculate expected return for every action in state s (P[s][a])\n",
    "                action_prob = policy[s][action] #probability of taking state \"a\" in state \"s\" (policy pi)\n",
    "                for real_action in policy[s].keys(): #the selected action of the policy is not the real action taken\n",
    "                    step_dynamics_prob = step_dynamics[s][action][real_action] \n",
    "            \n",
    "                    grid.set_state(s)\n",
    "                    reward = grid.move(real_action) #inmmediate reward of doing a in s\n",
    "                \n",
    "                    #bellman equation\n",
    "                    new_value += action_prob*step_dynamics_prob*(reward + gamma*V[grid.current_state()]) \n",
    "                \n",
    "                \n",
    "            V[s] = new_value #store the new value after bellman equation\n",
    "            \n",
    "            max_change = max(max_change,np.abs(old_value  - new_value )) #calc max change to break from loop\n",
    "            \n",
    "        \n",
    "        if max_change < convergence_threshold:\n",
    "            break\n",
    "            \n",
    "    return V\n",
    "\n",
    "print(\"Evaluation of initial random policy:\")\n",
    "iterative_policy_evaluation(policy_probs,0.9,CONVERGENCE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement policy improvement:\n",
    "Function that receives the state-value of a policy(found using iterative policy evaluation) and returns an improved policy by selecting for every state the action that maximizes it's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement(policy,V,gamma =1 ):\n",
    "    import operator #to do the argmax with dicts\n",
    "    \n",
    "    new_policy = {}\n",
    "    step_dynamics = create_step_dynamics(policy)\n",
    "    \n",
    "    for s in grid.actions.keys():\n",
    "        Qsa =defaultdict(lambda:0)\n",
    "        new_policy[s] = {}\n",
    "        \n",
    "        for action in policy[s].keys(): #actions given by policy(all actions)\n",
    "            \n",
    "            for real_action in policy[s].keys(): #get the probability of real action\n",
    "                step_dynamics_prob = step_dynamics[s][action][real_action]\n",
    "            \n",
    "                grid.set_state(s)\n",
    "                reward = grid.move(real_action) #reward of the real action taken\n",
    "                weighted =  step_dynamics_prob*(reward + gamma*V[grid.current_state()]) \n",
    "                \n",
    "                \n",
    "                Qsa[action] +=  weighted \n",
    "            \n",
    "            new_policy[s][action] = 0.0 #make sure all actions are in dictionary but only one will be best action\n",
    "            \n",
    "        state_best_action = max(Qsa.items(), key=operator.itemgetter(1))[0] #equivalent to argmax(Qsa) but for dicts\n",
    "        \n",
    "           \n",
    "        new_policy[s][state_best_action] = 1.0 \n",
    "        \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement policy iteration combining iterative policy evaluation and policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(initial_policy,gamma=1,convergence_threshold = 0.1):\n",
    "    policy_stable = False\n",
    "    policy = initial_policy\n",
    "    V = defaultdict(lambda :0)\n",
    "    \n",
    "    while not policy_stable:\n",
    "        V = iterative_policy_evaluation(policy,gamma,convergence_threshold)\n",
    "        improved_policy = policy_improvement(policy,V,gamma)\n",
    "        \n",
    "        \n",
    "        if equal_policies(policy,improved_policy):\n",
    "            policy_stable = True\n",
    "            \n",
    "        policy = improved_policy\n",
    "        \n",
    "        \n",
    "    return policy,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run policy iteration in the random initial policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy,V = policy_iteration(policy_probs,0.9,CONVERGENCE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned policy is:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned policy is:\")\n",
    "det_policy ={}\n",
    "for state in policy.keys():\n",
    "    for action in policy[state].keys():\n",
    "        if policy[state][action] == 1.0:\n",
    "            det_policy[state] = action\n",
    "\n",
    "print_policy(det_policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state-values of learned  policy are:\n",
      "--------------------------\n",
      "-4.52|-2.95|-0.86| 0.00|\n",
      "--------------------------\n",
      "-5.57| 0.00|-1.94| 0.00|\n",
      "--------------------------\n",
      "-5.76|-4.88|-3.44|-2.17|\n"
     ]
    }
   ],
   "source": [
    "print(\"The state-values of learned  policy are:\")\n",
    "print_values(grid=grid,V=V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "* Since theres negative rewards for every step, the expected value of every state is negative\n",
    "* The best state-value corresponds to the state closer to the winning state\n",
    "* The agent is encouraged to go to an end-state fast, even if it's the losing state because it learns to avoid the negative step rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
