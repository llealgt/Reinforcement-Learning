{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "step_count = 1\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    step_count+=1\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score),\" steps \",step_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 999999 #\n",
    "MAX_STEPS = 200000\n",
    "START_EPSILON = 1.0\n",
    "END_EPSILON = 0.1\n",
    "EPSILON_DECAY = 0.999\n",
    "UPDATE_TARGET_NETWORK_EVERY = 50\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "TRAIN_EVERY  = 4\n",
    "BATCH_SIZE = 32\n",
    "PRINT_EVERY = 10\n",
    "LEARNING_RATE = 0.00025\n",
    "VISUALIZE_EVERY = 100 #visualize the agent in the unity environment\n",
    "\n",
    "DESIRED_AVERAGE = 13\n",
    "DESIRED_EPISODES_AVERAGE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples for experience replay\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        #self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #assert len(hidden_sizes) > 0\n",
    "        self.fc1 = nn.Linear(state_size,50)\n",
    "        #self.hidden_layers = [] # list of nn.Linear\n",
    "        \n",
    "        #for i,size in enumerate(hidden_sizes[1:]):\n",
    "        #    self.hidden_layers.append(nn.Linear(hidden_sizes[i],size))\n",
    "         \n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.output_layer = nn.Linear(50,action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self,state_size, action_size,lr=0.001):\n",
    "        print(\"agent created\")\n",
    "        self.action_size = action_size\n",
    "        self.q_network_local = QNetwork(state_size,action_size).to(device) #network used to make desicion and with online updates\n",
    "        self.q_network_target = QNetwork(state_size,action_size).to(device) #network to calclate the target (copy of local)\n",
    "        self.q_network_target.eval()\n",
    "        self.optimizer = optim.Adam(self.q_network_local.parameters(),lr =lr) #TODO:ADD lr as parameter\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(action_size,REPLAY_BUFFER_SIZE) #TODO: make them variables of constructor\n",
    "        \n",
    "        self.explore_count = 0\n",
    "        self.exploit_count = 0\n",
    "        \n",
    "    def act(self,state,epsilon=0.0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        random_sampled = np.random.random()\n",
    "        \n",
    "        #explore\n",
    "        if random_sampled <= epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "            self.explore_count += 1 \n",
    "        else: #exploit\n",
    "            self.q_network_local.eval()\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network_local(state)\n",
    "                \n",
    "            greedy_action = np.argmax(q_values.cpu().data.numpy())   \n",
    "            action = greedy_action\n",
    "            self.exploit_count += 1\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for target_param, local_param in zip(self.q_network_target.parameters(), self.q_network_local.parameters()):\n",
    "            target_param.data.copy_(local_param.data )\n",
    "            \n",
    "    def train(self,batch_size):\n",
    "        (states, actions, rewards, next_states, dones) = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            local_max_action = self.q_network_local(next_states).detach().argmax(1).unsqueeze(1)\n",
    "            current_target  = rewards + 0.99*self.q_network_target(next_states).gather(dim=1,index=local_max_action)\n",
    "            \n",
    "        self.q_network_local.train()\n",
    "        current_estimate =  self.q_network_local(states).gather(1,actions)\n",
    "        \n",
    "        mse_loss = F.mse_loss(current_target,current_estimate)\n",
    "        self.optimizer.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def reset_explore_exploit_counts(self):\n",
    "        self.explore_count = 0\n",
    "        self.exploit_count = 0\n",
    "        \n",
    "    def get_explore_explit_counts(self):\n",
    "        return str((self.explore_count,self.exploit_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent created\n",
      "Episode: 0  steps: 300  episode reward: 0.0  epsilon: 1.0  explore,exploit counts: (300, 0)\n",
      "Episode: 10  steps: 300  episode reward: 2.0  epsilon: 0.9900448802097482  explore,exploit counts: (297, 3)\n",
      "Episode: 20  steps: 300  episode reward: 1.0  epsilon: 0.9801888648295347  explore,exploit counts: (293, 7)\n",
      "Episode: 30  steps: 300  episode reward: 0.0  epsilon: 0.9704309672630859  explore,exploit counts: (291, 9)\n",
      "Episode: 40  steps: 300  episode reward: 1.0  epsilon: 0.9607702107358118  explore,exploit counts: (292, 8)\n",
      "Episode: 50  steps: 300  episode reward: 1.0  epsilon: 0.9512056281970315  explore,exploit counts: (285, 15)\n",
      "Episode: 60  steps: 300  episode reward: 0.0  epsilon: 0.9417362622231683  explore,exploit counts: (275, 25)\n",
      "Episode: 70  steps: 300  episode reward: 1.0  epsilon: 0.9323611649219127  explore,exploit counts: (282, 18)\n",
      "Episode: 80  steps: 300  episode reward: -1.0  epsilon: 0.9230793978373364  explore,exploit counts: (279, 21)\n",
      "Episode: 90  steps: 300  episode reward: -1.0  epsilon: 0.9138900318559524  explore,exploit counts: (273, 27)\n",
      "Episode: 100  steps: 300  episode reward: 1.0  epsilon: 0.9047921471137096  explore,exploit counts: (275, 25)\n",
      "Episode: 110  steps: 300  episode reward: -1.0  epsilon: 0.8957848329039134  explore,exploit counts: (260, 40)\n",
      "Episode: 120  steps: 300  episode reward: 0.0  epsilon: 0.8868671875860644  explore,exploit counts: (266, 34)\n",
      "Episode: 130  steps: 300  episode reward: 0.0  epsilon: 0.8780383184956015  explore,exploit counts: (279, 21)\n",
      "Episode: 140  steps: 300  episode reward: 0.0  epsilon: 0.8692973418545467  explore,exploit counts: (257, 43)\n",
      "Episode: 150  steps: 300  episode reward: 2.0  epsilon: 0.8606433826830369  explore,exploit counts: (240, 60)\n",
      "Episode: 160  steps: 300  episode reward: 3.0  epsilon: 0.8520755747117399  explore,exploit counts: (249, 51)\n",
      "Episode: 170  steps: 300  episode reward: -1.0  epsilon: 0.8435930602951368  explore,exploit counts: (249, 51)\n",
      "Episode: 180  steps: 300  episode reward: 2.0  epsilon: 0.8351949903256736  explore,exploit counts: (244, 56)\n",
      "Episode: 190  steps: 300  episode reward: 0.0  epsilon: 0.8268805241487632  explore,exploit counts: (251, 49)\n",
      "Episode: 200  steps: 300  episode reward: 0.0  epsilon: 0.818648829478636  explore,exploit counts: (237, 63)\n",
      "Episode: 210  steps: 300  episode reward: 0.0  epsilon: 0.8104990823150267  explore,exploit counts: (247, 53)\n",
      "Episode: 220  steps: 300  episode reward: 1.0  epsilon: 0.8024304668606914  explore,exploit counts: (241, 59)\n",
      "Episode: 230  steps: 300  episode reward: 2.0  epsilon: 0.7944421754397457  explore,exploit counts: (231, 69)\n",
      "Episode: 240  steps: 300  episode reward: 3.0  epsilon: 0.7865334084168147  explore,exploit counts: (223, 77)\n",
      "Episode: 250  steps: 300  episode reward: 1.0  epsilon: 0.7787033741169904  explore,exploit counts: (231, 69)\n",
      "Episode: 260  steps: 300  episode reward: 1.0  epsilon: 0.7709512887465825  explore,exploit counts: (235, 65)\n",
      "Episode: 270  steps: 300  episode reward: 1.0  epsilon: 0.7632763763146613  explore,exploit counts: (233, 67)\n",
      "Episode: 280  steps: 300  episode reward: 0.0  epsilon: 0.7556778685553796  explore,exploit counts: (243, 57)\n",
      "Episode: 290  steps: 300  episode reward: 2.0  epsilon: 0.7481550048510686  explore,exploit counts: (223, 77)\n",
      "Episode: 300  steps: 300  episode reward: 2.0  epsilon: 0.7407070321560997  explore,exploit counts: (212, 88)\n",
      "Episode: 310  steps: 300  episode reward: 1.0  epsilon: 0.7333332049215037  explore,exploit counts: (228, 72)\n",
      "Episode: 320  steps: 300  episode reward: 4.0  epsilon: 0.7260327850203407  explore,exploit counts: (221, 79)\n",
      "Episode: 330  steps: 300  episode reward: 2.0  epsilon: 0.7188050416738131  explore,exploit counts: (216, 84)\n",
      "Episode: 340  steps: 300  episode reward: 1.0  epsilon: 0.7116492513781133  explore,exploit counts: (206, 94)\n",
      "Episode: 350  steps: 300  episode reward: 5.0  epsilon: 0.704564697832001  explore,exploit counts: (202, 98)\n",
      "Episode: 360  steps: 300  episode reward: 0.0  epsilon: 0.6975506718651011  explore,exploit counts: (209, 91)\n",
      "Episode: 370  steps: 300  episode reward: 2.0  epsilon: 0.6906064713669134  explore,exploit counts: (199, 101)\n",
      "Episode: 380  steps: 300  episode reward: 1.0  epsilon: 0.6837314012165328  explore,exploit counts: (213, 87)\n",
      "Episode: 390  steps: 300  episode reward: 2.0  epsilon: 0.6769247732130653  explore,exploit counts: (204, 96)\n",
      "Episode: 400  steps: 300  episode reward: 0.0  epsilon: 0.6701859060067403  explore,exploit counts: (196, 104)\n",
      "Episode: 410  steps: 300  episode reward: 2.0  epsilon: 0.6635141250307047  explore,exploit counts: (202, 98)\n",
      "Episode: 420  steps: 300  episode reward: 3.0  epsilon: 0.6569087624334999  explore,exploit counts: (213, 87)\n",
      "Episode: 430  steps: 300  episode reward: 3.0  epsilon: 0.6503691570122084  explore,exploit counts: (200, 100)\n",
      "Episode: 440  steps: 300  episode reward: 5.0  epsilon: 0.6438946541462667  explore,exploit counts: (190, 110)\n",
      "Episode: 450  steps: 300  episode reward: 0.0  epsilon: 0.6374846057319378  explore,exploit counts: (197, 103)\n",
      "Episode: 460  steps: 300  episode reward: 3.0  epsilon: 0.6311383701174348  explore,exploit counts: (182, 118)\n",
      "Episode: 470  steps: 300  episode reward: 2.0  epsilon: 0.6248553120386914  explore,exploit counts: (198, 102)\n",
      "Episode: 480  steps: 300  episode reward: 5.0  epsilon: 0.6186348025557711  explore,exploit counts: (180, 120)\n",
      "Episode: 490  steps: 300  episode reward: 7.0  epsilon: 0.6124762189899097  explore,exploit counts: (196, 104)\n",
      "Episode: 500  steps: 300  episode reward: 4.0  epsilon: 0.6063789448611848  explore,exploit counts: (187, 113)\n",
      "Episode: 510  steps: 300  episode reward: 8.0  epsilon: 0.6003423698268052  explore,exploit counts: (182, 118)\n",
      "Episode: 520  steps: 300  episode reward: 2.0  epsilon: 0.5943658896200158  explore,exploit counts: (178, 122)\n",
      "Episode: 530  steps: 300  episode reward: 4.0  epsilon: 0.5884489059896089  explore,exploit counts: (177, 123)\n",
      "Episode: 540  steps: 300  episode reward: 9.0  epsilon: 0.5825908266400397  explore,exploit counts: (171, 129)\n",
      "Episode: 550  steps: 300  episode reward: 1.0  epsilon: 0.5767910651721362  explore,exploit counts: (170, 130)\n",
      "Episode: 560  steps: 300  episode reward: 2.0  epsilon: 0.5710490410244006  explore,exploit counts: (160, 140)\n",
      "Episode: 570  steps: 300  episode reward: 4.0  epsilon: 0.5653641794148941  explore,exploit counts: (175, 125)\n",
      "Episode: 580  steps: 300  episode reward: 5.0  epsilon: 0.5597359112837013  explore,exploit counts: (174, 126)\n",
      "Episode: 590  steps: 300  episode reward: 3.0  epsilon: 0.5541636732359665  explore,exploit counts: (176, 124)\n",
      "Episode: 600  steps: 300  episode reward: 3.0  epsilon: 0.5486469074854965  explore,exploit counts: (168, 132)\n",
      "Episode: 610  steps: 300  episode reward: 4.0  epsilon: 0.5431850617989273  explore,exploit counts: (164, 136)\n",
      "Episode: 620  steps: 300  episode reward: 2.0  epsilon: 0.5377775894404436  explore,exploit counts: (164, 136)\n",
      "Episode: 630  steps: 300  episode reward: 3.0  epsilon: 0.532423949117051  explore,exploit counts: (146, 154)\n",
      "Episode: 640  steps: 300  episode reward: 8.0  epsilon: 0.5271236049243919  explore,exploit counts: (172, 128)\n",
      "Episode: 650  steps: 300  episode reward: 4.0  epsilon: 0.5218760262931003  explore,exploit counts: (155, 145)\n",
      "Episode: 660  steps: 300  episode reward: 6.0  epsilon: 0.5166806879356919  explore,exploit counts: (152, 148)\n",
      "Episode: 670  steps: 300  episode reward: 5.0  epsilon: 0.5115370697939825  explore,exploit counts: (132, 168)\n",
      "Episode: 680  steps: 300  episode reward: 8.0  epsilon: 0.506444656987029  explore,exploit counts: (137, 163)\n",
      "Episode: 690  steps: 300  episode reward: 9.0  epsilon: 0.5014029397595902  explore,exploit counts: (131, 169)\n",
      "Episode: 700  steps: 300  episode reward: 7.0  epsilon: 0.4964114134310989  explore,exploit counts: (151, 149)\n",
      "Episode: 710  steps: 300  episode reward: 6.0  epsilon: 0.4914695783451441  explore,exploit counts: (161, 139)\n",
      "Episode: 720  steps: 300  episode reward: 3.0  epsilon: 0.4865769398194536  explore,exploit counts: (154, 146)\n",
      "Episode: 730  steps: 300  episode reward: 4.0  epsilon: 0.48173300809637676  explore,exploit counts: (162, 138)\n",
      "Episode: 740  steps: 300  episode reward: 4.0  epsilon: 0.4769372982938589  explore,exploit counts: (137, 163)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 750  steps: 300  episode reward: 7.0  epsilon: 0.47218933035690447  explore,exploit counts: (140, 160)\n",
      "Episode: 760  steps: 300  episode reward: 6.0  epsilon: 0.46748862900952265  explore,exploit counts: (160, 140)\n",
      "Episode: 770  steps: 300  episode reward: 5.0  epsilon: 0.46283472370715234  explore,exploit counts: (143, 157)\n",
      "Episode: 780  steps: 300  episode reward: 6.0  epsilon: 0.45822714858955943  explore,exploit counts: (147, 153)\n",
      "Episode: 790  steps: 300  episode reward: 7.0  epsilon: 0.4536654424342049  explore,exploit counts: (141, 159)\n",
      "Episode: 800  steps: 300  episode reward: 8.0  epsilon: 0.4491491486100748  explore,exploit counts: (134, 166)\n",
      "Episode: 810  steps: 300  episode reward: 9.0  epsilon: 0.44467781503197196  explore,exploit counts: (135, 165)\n",
      "Episode: 820  steps: 300  episode reward: 4.0  epsilon: 0.4402509941152613  explore,exploit counts: (140, 160)\n",
      "Episode: 830  steps: 300  episode reward: 7.0  epsilon: 0.43586824273106645  explore,exploit counts: (143, 157)\n",
      "Episode: 840  steps: 300  episode reward: 8.0  epsilon: 0.43152912216191214  explore,exploit counts: (117, 183)\n",
      "Episode: 850  steps: 300  episode reward: 1.0  epsilon: 0.427233198057808  explore,exploit counts: (127, 173)\n",
      "Episode: 860  steps: 300  episode reward: 10.0  epsilon: 0.4229800403927701  explore,exploit counts: (132, 168)\n",
      "Episode: 870  steps: 300  episode reward: 6.0  epsilon: 0.41876922342177453  explore,exploit counts: (111, 189)\n",
      "Episode: 880  steps: 300  episode reward: 2.0  epsilon: 0.41460032563814003  explore,exploit counts: (135, 165)\n",
      "Episode: 890  steps: 300  episode reward: 12.0  epsilon: 0.410472929731335  explore,exploit counts: (122, 178)\n",
      "Episode: 900  steps: 300  episode reward: 8.0  epsilon: 0.4063866225452039  explore,exploit counts: (120, 180)\n",
      "Episode: 910  steps: 300  episode reward: 6.0  epsilon: 0.4023409950366106  explore,exploit counts: (133, 167)\n",
      "Episode: 920  steps: 300  episode reward: 8.0  epsilon: 0.398335642234492  explore,exploit counts: (111, 189)\n",
      "Episode: 930  steps: 300  episode reward: 8.0  epsilon: 0.3943701631993207  explore,exploit counts: (117, 183)\n",
      "Episode: 940  steps: 300  episode reward: 7.0  epsilon: 0.3904441609829703  explore,exploit counts: (122, 178)\n",
      "Episode: 950  steps: 300  episode reward: 3.0  epsilon: 0.3865572425889805  explore,exploit counts: (116, 184)\n",
      "Episode: 960  steps: 300  episode reward: 1.0  epsilon: 0.3827090189332178  explore,exploit counts: (116, 184)\n",
      "Episode: 970  steps: 300  episode reward: 7.0  epsilon: 0.3788991048049279  explore,exploit counts: (115, 185)\n",
      "Episode: 980  steps: 300  episode reward: 8.0  epsilon: 0.3751271188281757  explore,exploit counts: (128, 172)\n",
      "Episode: 990  steps: 300  episode reward: 8.0  epsilon: 0.3713926834236692  explore,exploit counts: (117, 183)\n",
      "Episode: 1000  steps: 300  episode reward: 10.0  epsilon: 0.3676954247709635  explore,exploit counts: (102, 198)\n",
      "Episode: 1010  steps: 300  episode reward: 8.0  epsilon: 0.36403497277104113  explore,exploit counts: (103, 197)\n",
      "Episode: 1020  steps: 300  episode reward: 9.0  epsilon: 0.36041096100926434  explore,exploit counts: (106, 194)\n",
      "Episode: 1030  steps: 300  episode reward: 11.0  epsilon: 0.3568230267186975  explore,exploit counts: (104, 196)\n",
      "Episode: 1040  steps: 300  episode reward: 5.0  epsilon: 0.35327081074379274  explore,exploit counts: (116, 184)\n",
      "Episode: 1050  steps: 300  episode reward: 8.0  epsilon: 0.349753957504439  explore,exploit counts: (97, 203)\n",
      "Episode: 1060  steps: 300  episode reward: 9.0  epsilon: 0.34627211496036764  explore,exploit counts: (98, 202)\n",
      "Episode: 1070  steps: 300  episode reward: 6.0  epsilon: 0.34282493457591334  explore,exploit counts: (108, 192)\n",
      "Episode: 1080  steps: 300  episode reward: 14.0  epsilon: 0.33941207128512485  explore,exploit counts: (101, 199)\n",
      "Episode: 1090  steps: 300  episode reward: 9.0  epsilon: 0.336033183457224  explore,exploit counts: (99, 201)\n",
      "Episode: 1100  steps: 300  episode reward: 8.0  epsilon: 0.33268793286240766  explore,exploit counts: (103, 197)\n",
      "Episode: 1110  steps: 300  episode reward: 3.0  epsilon: 0.3293759846379912  explore,exploit counts: (110, 190)\n",
      "Episode: 1120  steps: 300  episode reward: 2.0  epsilon: 0.3260970072548879  explore,exploit counts: (94, 206)\n",
      "Episode: 1130  steps: 300  episode reward: 9.0  epsilon: 0.32285067248442284  explore,exploit counts: (90, 210)\n",
      "Episode: 1140  steps: 300  episode reward: 7.0  epsilon: 0.31963665536547703  explore,exploit counts: (89, 211)\n",
      "Episode: 1150  steps: 300  episode reward: 6.0  epsilon: 0.31645463417195824  explore,exploit counts: (89, 211)\n",
      "Episode: 1160  steps: 300  episode reward: 15.0  epsilon: 0.31330429038059615  explore,exploit counts: (94, 206)\n",
      "Episode: 1170  steps: 300  episode reward: 13.0  epsilon: 0.3101853086390574  explore,exploit counts: (91, 209)\n",
      "Episode: 1180  steps: 300  episode reward: 4.0  epsilon: 0.30709737673437937  explore,exploit counts: (100, 200)\n",
      "Episode: 1190  steps: 300  episode reward: 13.0  epsilon: 0.30404018556171647  explore,exploit counts: (81, 219)\n",
      "Episode: 1200  steps: 300  episode reward: 7.0  epsilon: 0.3010134290933992  explore,exploit counts: (87, 213)\n",
      "Episode: 1210  steps: 300  episode reward: 10.0  epsilon: 0.29801680434829997  explore,exploit counts: (107, 193)\n",
      "Episode: 1220  steps: 300  episode reward: 9.0  epsilon: 0.2950500113615046  explore,exploit counts: (82, 218)\n",
      "Episode: 1230  steps: 300  episode reward: 5.0  epsilon: 0.29211275315428575  explore,exploit counts: (93, 207)\n",
      "Episode: 1240  steps: 300  episode reward: 11.0  epsilon: 0.2892047357043746  explore,exploit counts: (90, 210)\n",
      "Episode: 1250  steps: 300  episode reward: 14.0  epsilon: 0.28632566791652947  explore,exploit counts: (90, 210)\n",
      "Episode: 1260  steps: 300  episode reward: 13.0  epsilon: 0.28347526159339653  explore,exploit counts: (88, 212)\n",
      "Episode: 1270  steps: 300  episode reward: 6.0  epsilon: 0.2806532314066613  explore,exploit counts: (75, 225)\n",
      "Episode: 1280  steps: 300  episode reward: 12.0  epsilon: 0.27785929486848676  explore,exploit counts: (81, 219)\n",
      "Episode: 1290  steps: 300  episode reward: 10.0  epsilon: 0.2750931723032361  explore,exploit counts: (86, 214)\n",
      "Episode: 1300  steps: 300  episode reward: 5.0  epsilon: 0.27235458681947705  explore,exploit counts: (86, 214)\n",
      "Episode: 1310  steps: 300  episode reward: 9.0  epsilon: 0.2696432642822646  explore,exploit counts: (81, 219)\n",
      "Episode: 1320  steps: 300  episode reward: 13.0  epsilon: 0.2669589332857002  explore,exploit counts: (73, 227)\n",
      "Episode: 1330  steps: 300  episode reward: 10.0  epsilon: 0.2643013251257632  explore,exploit counts: (79, 221)\n",
      "Episode: 1340  steps: 300  episode reward: 12.0  epsilon: 0.261670173773414  explore,exploit counts: (82, 218)\n",
      "Episode: 1350  steps: 300  episode reward: 10.0  epsilon: 0.25906521584796366  explore,exploit counts: (64, 236)\n",
      "Episode: 1360  steps: 300  episode reward: 14.0  epsilon: 0.2564861905907097  explore,exploit counts: (92, 208)\n",
      "Episode: 1370  steps: 300  episode reward: 11.0  epsilon: 0.25393283983883386  explore,exploit counts: (81, 219)\n",
      "Episode: 1380  steps: 300  episode reward: 4.0  epsilon: 0.25140490799955945  explore,exploit counts: (76, 224)\n",
      "Episode: 1390  steps: 300  episode reward: 9.0  epsilon: 0.24890214202456656  explore,exploit counts: (87, 213)\n",
      "Episode: 1400  steps: 300  episode reward: 5.0  epsilon: 0.24642429138466176  explore,exploit counts: (76, 224)\n",
      "Episode: 1410  steps: 300  episode reward: 5.0  epsilon: 0.24397110804469957  explore,exploit counts: (65, 235)\n",
      "Episode: 1420  steps: 300  episode reward: 8.0  epsilon: 0.24154234643875414  explore,exploit counts: (66, 234)\n",
      "Episode: 1430  steps: 300  episode reward: 2.0  epsilon: 0.23913776344553783  explore,exploit counts: (77, 223)\n",
      "Episode: 1440  steps: 300  episode reward: 12.0  epsilon: 0.23675711836406457  explore,exploit counts: (64, 236)\n",
      "Episode: 1450  steps: 300  episode reward: 14.0  epsilon: 0.23440017288955545  explore,exploit counts: (57, 243)\n",
      "Episode: 1460  steps: 300  episode reward: 15.0  epsilon: 0.23206669108958414  explore,exploit counts: (75, 225)\n",
      "Episode: 1470  steps: 300  episode reward: 13.0  epsilon: 0.22975643938045995  explore,exploit counts: (63, 237)\n",
      "Episode: 1480  steps: 300  episode reward: 9.0  epsilon: 0.22746918650384582  explore,exploit counts: (73, 227)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1490  steps: 300  episode reward: 6.0  epsilon: 0.22520470350360888  explore,exploit counts: (70, 230)\n",
      "Episode: 1500  steps: 300  episode reward: 11.0  epsilon: 0.22296276370290227  explore,exploit counts: (61, 239)\n",
      "Episode: 1510  steps: 300  episode reward: 14.0  epsilon: 0.22074314268147424  explore,exploit counts: (69, 231)\n",
      "Episode: 1520  steps: 300  episode reward: 11.0  epsilon: 0.21854561825320348  explore,exploit counts: (72, 228)\n",
      "Episode: 1530  steps: 300  episode reward: 7.0  epsilon: 0.2163699704438582  explore,exploit counts: (62, 238)\n",
      "Episode: 1540  steps: 300  episode reward: 10.0  epsilon: 0.21421598146907636  explore,exploit counts: (69, 231)\n",
      "Episode: 1550  steps: 300  episode reward: 11.0  epsilon: 0.21208343571256533  explore,exploit counts: (66, 234)\n",
      "Episode: 1560  steps: 300  episode reward: 4.0  epsilon: 0.2099721197045186  explore,exploit counts: (62, 238)\n",
      "Episode: 1570  steps: 300  episode reward: 5.0  epsilon: 0.207881822100247  explore,exploit counts: (71, 229)\n",
      "Episode: 1580  steps: 300  episode reward: 9.0  epsilon: 0.20581233365902327  explore,exploit counts: (64, 236)\n",
      "Episode: 1590  steps: 300  episode reward: 13.0  epsilon: 0.20376344722313644  explore,exploit counts: (57, 243)\n",
      "Episode: 1600  steps: 300  episode reward: 11.0  epsilon: 0.20173495769715546  explore,exploit counts: (54, 246)\n",
      "Episode: 1610  steps: 300  episode reward: 11.0  epsilon: 0.19972666202739892  explore,exploit counts: (58, 242)\n",
      "Episode: 1620  steps: 300  episode reward: 10.0  epsilon: 0.197738359181609  explore,exploit counts: (68, 232)\n",
      "Episode: 1630  steps: 300  episode reward: 11.0  epsilon: 0.19576985012882828  explore,exploit counts: (54, 246)\n",
      "Episode: 1640  steps: 300  episode reward: 13.0  epsilon: 0.19382093781947612  explore,exploit counts: (60, 240)\n",
      "Episode: 1650  steps: 300  episode reward: 10.0  epsilon: 0.19189142716562432  explore,exploit counts: (56, 244)\n",
      "Episode: 1660  steps: 300  episode reward: 1.0  epsilon: 0.1899811250214681  explore,exploit counts: (62, 238)\n",
      "Episode: 1670  steps: 300  episode reward: 15.0  epsilon: 0.18808984016399252  explore,exploit counts: (49, 251)\n",
      "Episode: 1680  steps: 300  episode reward: 12.0  epsilon: 0.1862173832738307  explore,exploit counts: (54, 246)\n",
      "Episode: 1690  steps: 300  episode reward: 5.0  epsilon: 0.18436356691631248  explore,exploit counts: (59, 241)\n",
      "Episode: 1700  steps: 300  episode reward: 18.0  epsilon: 0.18252820552270246  explore,exploit counts: (60, 240)\n",
      "Episode: 1710  steps: 300  episode reward: 19.0  epsilon: 0.18071111537162424  explore,exploit counts: (52, 248)\n",
      "Episode: 1720  steps: 300  episode reward: 15.0  epsilon: 0.17891211457066972  explore,exploit counts: (57, 243)\n",
      "Episode: 1730  steps: 300  episode reward: 6.0  epsilon: 0.17713102303819148  explore,exploit counts: (41, 259)\n",
      "Episode: 1740  steps: 300  episode reward: 10.0  epsilon: 0.17536766248527644  explore,exploit counts: (49, 251)\n",
      "Episode: 1750  steps: 300  episode reward: 13.0  epsilon: 0.17362185639789907  explore,exploit counts: (47, 253)\n",
      "Episode: 1760  steps: 300  episode reward: 11.0  epsilon: 0.1718934300192521  explore,exploit counts: (54, 246)\n",
      "Episode: 1770  steps: 300  episode reward: 15.0  epsilon: 0.17018221033225317  explore,exploit counts: (49, 251)\n",
      "Episode: 1780  steps: 300  episode reward: 8.0  epsilon: 0.1684880260422258  explore,exploit counts: (57, 243)\n",
      "Episode: 1790  steps: 300  episode reward: 1.0  epsilon: 0.1668107075597524  explore,exploit counts: (52, 248)\n",
      "Episode: 1800  steps: 300  episode reward: 7.0  epsilon: 0.1651500869836984  explore,exploit counts: (44, 256)\n",
      "Episode: 1810  steps: 300  episode reward: 11.0  epsilon: 0.16350599808440516  explore,exploit counts: (45, 255)\n",
      "Episode: 1820  steps: 300  episode reward: 5.0  epsilon: 0.16187827628705023  explore,exploit counts: (48, 252)\n",
      "Episode: 1830  steps: 300  episode reward: 15.0  epsilon: 0.16026675865517317  explore,exploit counts: (51, 249)\n",
      "Episode: 1840  steps: 300  episode reward: 11.0  epsilon: 0.15867128387436555  explore,exploit counts: (53, 247)\n",
      "Episode: 1850  steps: 300  episode reward: 14.0  epsilon: 0.1570916922361232  explore,exploit counts: (54, 246)\n",
      "Episode: 1860  steps: 300  episode reward: 11.0  epsilon: 0.15552782562185924  explore,exploit counts: (44, 256)\n",
      "Episode: 1870  steps: 300  episode reward: 13.0  epsilon: 0.15397952748707622  explore,exploit counts: (51, 249)\n",
      "Episode: 1880  steps: 300  episode reward: 13.0  epsilon: 0.15244664284569598  explore,exploit counts: (54, 246)\n",
      "Episode: 1890  steps: 300  episode reward: 9.0  epsilon: 0.15092901825454533  explore,exploit counts: (46, 254)\n",
      "Episode: 1900  steps: 300  episode reward: 15.0  epsilon: 0.14942650179799613  explore,exploit counts: (40, 260)\n",
      "Episode: 1910  steps: 300  episode reward: 7.0  epsilon: 0.1479389430727588  explore,exploit counts: (44, 256)\n",
      "Episode: 1920  steps: 300  episode reward: 19.0  epsilon: 0.14646619317282628  explore,exploit counts: (43, 257)\n",
      "Episode: 1930  steps: 300  episode reward: 13.0  epsilon: 0.14500810467456865  explore,exploit counts: (41, 259)\n",
      "Episode: 1940  steps: 300  episode reward: 13.0  epsilon: 0.1435645316219759  explore,exploit counts: (45, 255)\n",
      "Episode: 1950  steps: 300  episode reward: 10.0  epsilon: 0.14213532951204774  explore,exploit counts: (37, 263)\n",
      "Episode: 1960  steps: 300  episode reward: 13.0  epsilon: 0.14072035528032842  explore,exploit counts: (51, 249)\n",
      "Episode: 1970  steps: 300  episode reward: 11.0  epsilon: 0.13931946728658592  explore,exploit counts: (34, 266)\n",
      "Episode: 1980  steps: 300  episode reward: 11.0  epsilon: 0.13793252530063388  explore,exploit counts: (31, 269)\n",
      "Episode: 1990  steps: 300  episode reward: 9.0  epsilon: 0.1365593904882941  explore,exploit counts: (40, 260)\n",
      "Episode: 2000  steps: 300  episode reward: 13.0  epsilon: 0.1351999253974994  explore,exploit counts: (34, 266)\n",
      "Episode: 2010  steps: 300  episode reward: 9.0  epsilon: 0.13385399394453418  explore,exploit counts: (42, 258)\n",
      "Solved in  2012  episodes \n",
      "Last  100  average reward: 13.04\n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name=\"./Banana_Linux/Banana.x86_64\")\n",
    "agent = Agent(state_size,action_size,LEARNING_RATE)\n",
    "\n",
    "episodes_rewards = []\n",
    "solved = False\n",
    "epsilon = START_EPSILON\n",
    "for episode in range(EPISODES):\n",
    "    finished = False\n",
    "    step_count = 0\n",
    "    episode_score = 0\n",
    "    \n",
    "    agent.reset_explore_exploit_counts()\n",
    "    \n",
    "    train_mode = not (episode % VISUALIZE_EVERY == 0)\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    while not finished: #and step_count < MAX_STEPS:\n",
    "        action = agent.act(state,epsilon) \n",
    "\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        finished = env_info.local_done[0]\n",
    "        \n",
    "        # add experience tuple to experience replay buffer\n",
    "        agent.replay_buffer.add(state,action,reward,next_state,finished)\n",
    "        episode_score += reward\n",
    "        state = next_state\n",
    "        \n",
    "        # create a copy of local network to target network\n",
    "        if step_count % UPDATE_TARGET_NETWORK_EVERY == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "            \n",
    "        if step_count % TRAIN_EVERY == 0 and len(agent.replay_buffer) >=  BATCH_SIZE:\n",
    "            agent.train(BATCH_SIZE)\n",
    "            \n",
    "        step_count += 1\n",
    "        \n",
    "    if episode % PRINT_EVERY == 0:\n",
    "        print(\"Episode:\",episode, \" steps:\",step_count, \" episode reward:\",episode_score, \" epsilon:\",epsilon,\" explore,exploit counts:\",agent.get_explore_explit_counts())\n",
    "    \n",
    "    episodes_rewards.append(episode_score)\n",
    "    last_n_episode_rewards = np.mean(episodes_rewards[-DESIRED_EPISODES_AVERAGE:])\n",
    "    \n",
    "    if last_n_episode_rewards >= DESIRED_AVERAGE:\n",
    "        print(\"Solved in \",episode, \" episodes \")\n",
    "        checkpoint_name = \"checkpoint_solved_\"+str(episode)+\".pth\"\n",
    "        torch.save(agent.q_network_local.state_dict(), checkpoint_name)\n",
    "        solved = True\n",
    "        break\n",
    "    \n",
    "    epsilon = max(END_EPSILON, EPSILON_DECAY*epsilon)\n",
    "    \n",
    "if not solved:\n",
    "    checkpoint_name = \"checkpoint_not_solved_\"+str(episode)+\".pth\"\n",
    "    torch.save(agent.q_network_local.state_dict(), checkpoint_name)\n",
    "\n",
    "print(\"Last \", DESIRED_EPISODES_AVERAGE,\" average reward:\",last_n_episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(episodes_rewards)), episodes_rewards)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
