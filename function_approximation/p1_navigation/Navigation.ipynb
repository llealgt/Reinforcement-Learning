{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "step_count = 1\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    step_count+=1\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score),\" steps \",step_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 999999 #\n",
    "MAX_STEPS = 200000\n",
    "START_EPSILON = 1.0\n",
    "END_EPSILON = 0.01\n",
    "EPSILON_DECAY = 0.990\n",
    "UPDATE_TARGET_NETWORK_EVERY = 50\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "TRAIN_EVERY  = 4\n",
    "BATCH_SIZE = 32\n",
    "PRINT_EVERY = 10\n",
    "LEARNING_RATE = 0.00025\n",
    "VISUALIZE_EVERY = 100 #visualize the agent in the unity environment\n",
    "\n",
    "DESIRED_AVERAGE = 13\n",
    "DESIRED_EPISODES_AVERAGE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples for experience replay\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        #self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #assert len(hidden_sizes) > 0\n",
    "        self.fc1 = nn.Linear(state_size,100)\n",
    "        #self.hidden_layers = [] # list of nn.Linear\n",
    "        \n",
    "        #for i,size in enumerate(hidden_sizes[1:]):\n",
    "        #    self.hidden_layers.append(nn.Linear(hidden_sizes[i],size))\n",
    "         \n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.output_layer = nn.Linear(100,action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self,state_size, action_size,lr=0.001):\n",
    "        print(\"agent created\")\n",
    "        self.action_size = action_size\n",
    "        self.q_network_local = QNetwork(state_size,action_size).to(device) #network used to make desicion and with online updates\n",
    "        self.q_network_target = QNetwork(state_size,action_size).to(device) #network to calclate the target (copy of local)\n",
    "        self.q_network_target.eval()\n",
    "        self.optimizer = optim.Adam(self.q_network_local.parameters(),lr =lr) #TODO:ADD lr as parameter\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(action_size,REPLAY_BUFFER_SIZE) #TODO: make them variables of constructor\n",
    "        \n",
    "        self.explore_count = 0\n",
    "        self.exploit_count = 0\n",
    "        \n",
    "    def act(self,state,epsilon=0.0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        random_sampled = np.random.random()\n",
    "        \n",
    "        #explore\n",
    "        if random_sampled <= epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "            self.explore_count += 1 \n",
    "        else: #exploit\n",
    "            self.q_network_local.eval()\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network_local(state)\n",
    "                \n",
    "            greedy_action = np.argmax(q_values.cpu().data.numpy())   \n",
    "            action = greedy_action\n",
    "            self.exploit_count += 1\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for target_param, local_param in zip(self.q_network_target.parameters(), self.q_network_local.parameters()):\n",
    "            target_param.data.copy_(local_param.data )\n",
    "            \n",
    "    def train(self,batch_size):\n",
    "        (states, actions, rewards, next_states, dones) = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            local_max_action = self.q_network_local(next_states).detach().argmax(1).unsqueeze(1)\n",
    "            current_target  = rewards + 0.99*self.q_network_target(next_states).gather(dim=1,index=local_max_action)\n",
    "            \n",
    "        self.q_network_local.train()\n",
    "        current_estimate =  self.q_network_local(states).gather(1,actions)\n",
    "        \n",
    "        mse_loss = F.mse_loss(current_target,current_estimate)\n",
    "        self.optimizer.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def reset_explore_exploit_counts(self):\n",
    "        self.explore_count = 0\n",
    "        self.exploit_count = 0\n",
    "        \n",
    "    def get_explore_explit_counts(self):\n",
    "        return str((self.explore_count,self.exploit_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent created\n",
      "Episode: 0  steps: 300  episode reward: 0.0  epsilon: 1.0  explore,exploit counts: (300, 0)\n",
      "Episode: 10  steps: 300  episode reward: -1.0  epsilon: 0.9043820750088043  explore,exploit counts: (271, 29)\n",
      "Episode: 20  steps: 300  episode reward: 0.0  epsilon: 0.8179069375972307  explore,exploit counts: (242, 58)\n",
      "Episode: 30  steps: 300  episode reward: -2.0  epsilon: 0.7397003733882802  explore,exploit counts: (224, 76)\n",
      "Episode: 40  steps: 300  episode reward: 0.0  epsilon: 0.6689717585696803  explore,exploit counts: (191, 109)\n",
      "Episode: 50  steps: 300  episode reward: -1.0  epsilon: 0.6050060671375365  explore,exploit counts: (184, 116)\n",
      "Episode: 60  steps: 300  episode reward: 1.0  epsilon: 0.5471566423907612  explore,exploit counts: (174, 126)\n",
      "Episode: 70  steps: 300  episode reward: -2.0  epsilon: 0.49483865960020695  explore,exploit counts: (149, 151)\n",
      "Episode: 80  steps: 300  episode reward: 1.0  epsilon: 0.44752321376381066  explore,exploit counts: (131, 169)\n",
      "Episode: 90  steps: 300  episode reward: 0.0  epsilon: 0.4047319726783239  explore,exploit counts: (124, 176)\n",
      "Episode: 100  steps: 300  episode reward: -4.0  epsilon: 0.36603234127322926  explore,exploit counts: (111, 189)\n",
      "Episode: 110  steps: 300  episode reward: -2.0  epsilon: 0.33103308832101386  explore,exploit counts: (107, 193)\n",
      "Episode: 120  steps: 300  episode reward: -1.0  epsilon: 0.29938039131233124  explore,exploit counts: (93, 207)\n",
      "Episode: 130  steps: 300  episode reward: 0.0  epsilon: 0.270754259511994  explore,exploit counts: (88, 212)\n",
      "Episode: 140  steps: 300  episode reward: 0.0  epsilon: 0.24486529903492946  explore,exploit counts: (73, 227)\n",
      "Episode: 150  steps: 300  episode reward: -1.0  epsilon: 0.22145178723886094  explore,exploit counts: (60, 240)\n",
      "Episode: 160  steps: 300  episode reward: 2.0  epsilon: 0.20027702685748935  explore,exploit counts: (50, 250)\n",
      "Episode: 170  steps: 300  episode reward: 1.0  epsilon: 0.18112695312597027  explore,exploit counts: (41, 259)\n",
      "Episode: 180  steps: 300  episode reward: 3.0  epsilon: 0.16380796970808745  explore,exploit counts: (46, 254)\n",
      "Episode: 190  steps: 300  episode reward: 0.0  epsilon: 0.1481449915475795  explore,exploit counts: (42, 258)\n",
      "Episode: 200  steps: 300  episode reward: 2.0  epsilon: 0.13397967485796175  explore,exploit counts: (53, 247)\n",
      "Episode: 210  steps: 300  episode reward: 1.0  epsilon: 0.1211688163570484  explore,exploit counts: (40, 260)\n",
      "Episode: 220  steps: 300  episode reward: 3.0  epsilon: 0.10958290556334822  explore,exploit counts: (40, 260)\n",
      "Episode: 230  steps: 300  episode reward: 7.0  epsilon: 0.09910481551887473  explore,exploit counts: (32, 268)\n",
      "Episode: 240  steps: 300  episode reward: 6.0  epsilon: 0.08962861870232469  explore,exploit counts: (24, 276)\n",
      "Episode: 250  steps: 300  episode reward: 7.0  epsilon: 0.08105851616218133  explore,exploit counts: (21, 279)\n",
      "Episode: 260  steps: 300  episode reward: 12.0  epsilon: 0.07330786904388827  explore,exploit counts: (21, 279)\n",
      "Episode: 270  steps: 300  episode reward: 5.0  epsilon: 0.06629832272038537  explore,exploit counts: (13, 287)\n",
      "Episode: 280  steps: 300  episode reward: -1.0  epsilon: 0.05995901467146548  explore,exploit counts: (19, 281)\n",
      "Episode: 290  steps: 300  episode reward: 11.0  epsilon: 0.054225858104063294  explore,exploit counts: (15, 285)\n",
      "Episode: 300  steps: 300  episode reward: 5.0  epsilon: 0.04904089407128576  explore,exploit counts: (16, 284)\n",
      "Episode: 310  steps: 300  episode reward: 7.0  epsilon: 0.04435170554047638  explore,exploit counts: (18, 282)\n",
      "Episode: 320  steps: 300  episode reward: 2.0  epsilon: 0.04011088748687551  explore,exploit counts: (9, 291)\n",
      "Episode: 330  steps: 300  episode reward: 7.0  epsilon: 0.036275567655825146  explore,exploit counts: (11, 289)\n",
      "Episode: 340  steps: 300  episode reward: 6.0  epsilon: 0.03280697314869741  explore,exploit counts: (12, 288)\n",
      "Episode: 350  steps: 300  episode reward: 5.0  epsilon: 0.029670038450977095  explore,exploit counts: (11, 289)\n",
      "Episode: 360  steps: 300  episode reward: 4.0  epsilon: 0.026833050939885677  explore,exploit counts: (8, 292)\n",
      "Episode: 370  steps: 300  episode reward: 0.0  epsilon: 0.024267330287830756  explore,exploit counts: (6, 294)\n",
      "Episode: 380  steps: 300  episode reward: 13.0  epsilon: 0.02194693852063239  explore,exploit counts: (5, 295)\n",
      "Episode: 390  steps: 300  episode reward: 7.0  epsilon: 0.01984841779938018  explore,exploit counts: (9, 291)\n",
      "Episode: 400  steps: 300  episode reward: 1.0  epsilon: 0.017950553275045134  explore,exploit counts: (9, 291)\n",
      "Episode: 410  steps: 300  episode reward: 7.0  epsilon: 0.01623415861844141  explore,exploit counts: (6, 294)\n",
      "Episode: 420  steps: 300  episode reward: 3.0  epsilon: 0.014681882057368112  explore,exploit counts: (6, 294)\n",
      "Episode: 430  steps: 300  episode reward: 15.0  epsilon: 0.013278030960077106  explore,exploit counts: (2, 298)\n",
      "Episode: 440  steps: 300  episode reward: 11.0  epsilon: 0.01200841319170568  explore,exploit counts: (2, 298)\n",
      "Episode: 450  steps: 300  episode reward: 6.0  epsilon: 0.010860193639877886  explore,exploit counts: (1, 299)\n",
      "Episode: 460  steps: 300  episode reward: 12.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 470  steps: 300  episode reward: 12.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 480  steps: 300  episode reward: 12.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 490  steps: 300  episode reward: 11.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 500  steps: 300  episode reward: 6.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 510  steps: 300  episode reward: 5.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 520  steps: 300  episode reward: 5.0  epsilon: 0.01  explore,exploit counts: (2, 298)\n",
      "Episode: 530  steps: 300  episode reward: 13.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 540  steps: 300  episode reward: 11.0  epsilon: 0.01  explore,exploit counts: (2, 298)\n",
      "Episode: 550  steps: 300  episode reward: 12.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 560  steps: 300  episode reward: 9.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 570  steps: 300  episode reward: 17.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 580  steps: 300  episode reward: 13.0  epsilon: 0.01  explore,exploit counts: (5, 295)\n",
      "Episode: 590  steps: 300  episode reward: 14.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 600  steps: 300  episode reward: 7.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 610  steps: 300  episode reward: 7.0  epsilon: 0.01  explore,exploit counts: (7, 293)\n",
      "Episode: 620  steps: 300  episode reward: 2.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 630  steps: 300  episode reward: 6.0  epsilon: 0.01  explore,exploit counts: (2, 298)\n",
      "Episode: 640  steps: 300  episode reward: 13.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 650  steps: 300  episode reward: 14.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 660  steps: 300  episode reward: 13.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 670  steps: 300  episode reward: 14.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 680  steps: 300  episode reward: 10.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 690  steps: 300  episode reward: 16.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 700  steps: 300  episode reward: 14.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 710  steps: 300  episode reward: 10.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 720  steps: 300  episode reward: 7.0  epsilon: 0.01  explore,exploit counts: (2, 298)\n",
      "Episode: 730  steps: 300  episode reward: 10.0  epsilon: 0.01  explore,exploit counts: (0, 300)\n",
      "Episode: 740  steps: 300  episode reward: 14.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 750  steps: 300  episode reward: 17.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 760  steps: 300  episode reward: 9.0  epsilon: 0.01  explore,exploit counts: (5, 295)\n",
      "Episode: 770  steps: 300  episode reward: 11.0  epsilon: 0.01  explore,exploit counts: (5, 295)\n",
      "Episode: 780  steps: 300  episode reward: 19.0  epsilon: 0.01  explore,exploit counts: (2, 298)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 790  steps: 300  episode reward: 20.0  epsilon: 0.01  explore,exploit counts: (5, 295)\n",
      "Episode: 800  steps: 300  episode reward: 11.0  epsilon: 0.01  explore,exploit counts: (0, 300)\n",
      "Episode: 810  steps: 300  episode reward: 17.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 820  steps: 300  episode reward: 13.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 830  steps: 300  episode reward: 11.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 840  steps: 300  episode reward: 5.0  epsilon: 0.01  explore,exploit counts: (7, 293)\n",
      "Episode: 850  steps: 300  episode reward: 12.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 860  steps: 300  episode reward: 22.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 870  steps: 300  episode reward: 19.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 880  steps: 300  episode reward: 2.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 890  steps: 300  episode reward: 19.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 900  steps: 300  episode reward: 15.0  epsilon: 0.01  explore,exploit counts: (0, 300)\n",
      "Episode: 910  steps: 300  episode reward: 15.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 920  steps: 300  episode reward: 19.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 930  steps: 300  episode reward: 19.0  epsilon: 0.01  explore,exploit counts: (2, 298)\n",
      "Episode: 940  steps: 300  episode reward: 11.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 950  steps: 300  episode reward: 14.0  epsilon: 0.01  explore,exploit counts: (0, 300)\n",
      "Episode: 960  steps: 300  episode reward: 10.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 970  steps: 300  episode reward: 12.0  epsilon: 0.01  explore,exploit counts: (1, 299)\n",
      "Episode: 980  steps: 300  episode reward: 18.0  epsilon: 0.01  explore,exploit counts: (3, 297)\n",
      "Episode: 990  steps: 300  episode reward: 15.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Episode: 1000  steps: 300  episode reward: 19.0  epsilon: 0.01  explore,exploit counts: (4, 296)\n",
      "Solved in  1007  episodes \n",
      "Last  100  average reward: 13.01\n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name=\"./Banana_Linux/Banana.x86_64\")\n",
    "agent = Agent(state_size,action_size,LEARNING_RATE)\n",
    "\n",
    "episodes_rewards = []\n",
    "solved = False\n",
    "epsilon = START_EPSILON\n",
    "for episode in range(EPISODES):\n",
    "    finished = False\n",
    "    step_count = 0\n",
    "    episode_score = 0\n",
    "    \n",
    "    agent.reset_explore_exploit_counts()\n",
    "    \n",
    "    train_mode = not (episode % VISUALIZE_EVERY == 0)\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    while not finished: #and step_count < MAX_STEPS:\n",
    "        action = agent.act(state,epsilon) \n",
    "\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        finished = env_info.local_done[0]\n",
    "        \n",
    "        # add experience tuple to experience replay buffer\n",
    "        agent.replay_buffer.add(state,action,reward,next_state,finished)\n",
    "        episode_score += reward\n",
    "        state = next_state\n",
    "        \n",
    "        # create a copy of local network to target network\n",
    "        if step_count % UPDATE_TARGET_NETWORK_EVERY == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "            \n",
    "        if step_count % TRAIN_EVERY == 0 and len(agent.replay_buffer) >=  BATCH_SIZE:\n",
    "            agent.train(BATCH_SIZE)\n",
    "            \n",
    "        step_count += 1\n",
    "        \n",
    "    if episode % PRINT_EVERY == 0:\n",
    "        print(\"Episode:\",episode, \" steps:\",step_count, \" episode reward:\",episode_score, \" epsilon:\",epsilon,\" explore,exploit counts:\",agent.get_explore_explit_counts())\n",
    "    \n",
    "    episodes_rewards.append(episode_score)\n",
    "    last_n_episode_rewards = np.mean(episodes_rewards[-DESIRED_EPISODES_AVERAGE:])\n",
    "    \n",
    "    if last_n_episode_rewards >= DESIRED_AVERAGE:\n",
    "        print(\"Solved in \",episode, \" episodes \")\n",
    "        checkpoint_name = \"checkpoint_solved_\"+str(episode)+\".pth\"\n",
    "        torch.save(agent.q_network_local.state_dict(), checkpoint_name)\n",
    "        solved = True\n",
    "        break\n",
    "    \n",
    "    epsilon = max(END_EPSILON, EPSILON_DECAY*epsilon)\n",
    "    \n",
    "if not solved:\n",
    "    checkpoint_name = \"checkpoint_not_solved_\"+str(episode)+\".pth\"\n",
    "    torch.save(agent.q_network_local.state_dict(), checkpoint_name)\n",
    "\n",
    "print(\"Last \", DESIRED_EPISODES_AVERAGE,\" average reward:\",last_n_episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(episodes_rewards)), episodes_rewards)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
